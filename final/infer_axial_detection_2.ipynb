{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17fcf25f",
   "metadata": {
    "papermill": {
     "duration": 4.064069,
     "end_time": "2022-10-17T23:12:36.990186",
     "exception": false,
     "start_time": "2022-10-17T23:12:32.926117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import ast\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import pydicom as dicom\n",
    "import pylibjpeg\n",
    "\n",
    "effdet_path = \"../input/effdet-models/effdet\"\n",
    "sys.path.append(effdet_path)\n",
    "\n",
    "\n",
    "timm_path = \"../input/effdet-models/timm-pytorch-image-models\"\n",
    "sys.path.append(timm_path)\n",
    "import timm\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib import patches\n",
    "# import sklearn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "omega_path = \"../input/effdet-models/omegaconf\"\n",
    "sys.path.append(omega_path)\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "effunet_path = \"../input/effdet-models/efficientunet-pytorch-0.0.6\"\n",
    "sys.path.append(effunet_path)\n",
    "\n",
    "# from effdet import create_model\n",
    "\n",
    "import glob\n",
    "# import sklearn\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# import cv2\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "# from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# from sklearn import metrics, model_selection, preprocessing\n",
    "\n",
    "# from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "# pos_weight = torch.tensor(pos_weight)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d906cb",
   "metadata": {
    "papermill": {
     "duration": 0.01514,
     "end_time": "2022-10-17T23:12:37.012445",
     "exception": false,
     "start_time": "2022-10-17T23:12:36.997305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/root/autodl-tmp/cervical_spine/\"\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'train_axial_images_jpeg95')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6674d81-b7c2-4883-be1b-91f025f44ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_overall</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.6200</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.27262</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.21561</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.12351</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2.826.0.1.3680043.1363</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           patient_overall  C1  C2  C3  C4  C5  C6  C7\n",
       "StudyInstanceUID                                                      \n",
       "1.2.826.0.1.3680043.6200                 1   1   1   0   0   0   0   0\n",
       "1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0\n",
       "1.2.826.0.1.3680043.21561                1   0   1   0   0   0   0   0\n",
       "1.2.826.0.1.3680043.12351                0   0   0   0   0   0   0   0\n",
       "1.2.826.0.1.3680043.1363                 1   0   0   0   0   1   0   0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv')).set_index('StudyInstanceUID')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "243ff9ef",
   "metadata": {
    "papermill": {
     "duration": 0.01553,
     "end_time": "2022-10-17T23:12:37.034910",
     "exception": false,
     "start_time": "2022-10-17T23:12:37.019380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "segmentation_checkpoint=\"../input/effdet-models/axial_segmentation_effseg_132508-epoch-100.pth\"\n",
    "axial_det_checkpoint=\"../input/effdet-models/axial_detection_effdet_134352-epoch-52.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9751a21c",
   "metadata": {
    "papermill": {
     "duration": 0.017263,
     "end_time": "2022-10-17T23:12:37.191371",
     "exception": false,
     "start_time": "2022-10-17T23:12:37.174108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rescale_img_to_hu(dcm_ds):\n",
    "    \"\"\"Rescales the image to Hounsfield unit.\n",
    "    \"\"\"\n",
    "    return dcm_ds.pixel_array * dcm_ds.RescaleSlope + dcm_ds.RescaleIntercept\n",
    "\n",
    "def normalize_hu(data):\n",
    "    # normalize to 0-1\n",
    "    # return (data - data.min()) / data.max()\n",
    "    data = np.clip(data, a_min=-2242, a_max=2242) / 4484 + 0.5\n",
    "    return data\n",
    "\n",
    "def load_dicom(path):\n",
    "    \"\"\"\n",
    "    This supports loading both regular and compressed JPEG images. \n",
    "    See the first sell with `pip install` commands for the necessary dependencies\n",
    "    \"\"\"\n",
    "    ds=dicom.dcmread(path) \n",
    "\n",
    "    img = rescale_img_to_hu(ds)\n",
    "\n",
    "    return img, ds.PixelSpacing[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a82ea72",
   "metadata": {
    "papermill": {
     "duration": 0.084026,
     "end_time": "2022-10-17T23:12:37.282765",
     "exception": false,
     "start_time": "2022-10-17T23:12:37.198739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DcmDataSet(torch.utils.data.Dataset):    \n",
    "    def __init__(self, df, path, transforms=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.len = len(self.df)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        try:\n",
    "            s = self.df.iloc[i]\n",
    "            prev_s = self.df.iloc[i-1] if i > 0 else s\n",
    "            prev_s = s if prev_s.StudyInstanceUID != s.StudyInstanceUID else prev_s\n",
    "            \n",
    "            next_s = self.df.iloc[i+1] if i < (self.len-1) else s\n",
    "            next_s = s if next_s.StudyInstanceUID != s.StudyInstanceUID else next_s\n",
    "            \n",
    "            rpath = os.path.join(self.path, s.StudyInstanceUID, f'{prev_s.Slice}.jpeg')\n",
    "            gpath = os.path.join(self.path, s.StudyInstanceUID, f'{s.Slice}.jpeg')\n",
    "            bpath = os.path.join(self.path, s.StudyInstanceUID, f'{next_s.Slice}.jpeg')\n",
    "            \n",
    "            g, pixel_spacing = load_dicom(gpath) \n",
    "            r, _ = load_dicom(rpath)\n",
    "            b, _ = load_dicom(bpath)\n",
    "            \n",
    "            img = np.stack((r, g, b))\n",
    "            img = normalize_hu(img)\n",
    "            \n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            return None, None\n",
    "        \n",
    "        return img, pixel_spacing      \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "class DataTransform(nn.Module):\n",
    "    def __init__(self, image_size=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.Normalize(0.5, 0.5),\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transform(torch.as_tensor(x, dtype=torch.float))\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c57ff08f",
   "metadata": {
    "papermill": {
     "duration": 9.121351,
     "end_time": "2022-10-17T23:12:46.434785",
     "exception": false,
     "start_time": "2022-10-17T23:12:37.313434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/effdet-models/axial_segmentation_effseg_132508-epoch-100.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29253/2132450042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mseg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_axial_segmentation_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmentation_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_29253/2132450042.py\u001b[0m in \u001b[0;36mget_axial_segmentation_model\u001b[0;34m(checkpoint)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_efficientunet_b5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/effdet-models/axial_segmentation_effseg_132508-epoch-100.pth'"
     ]
    }
   ],
   "source": [
    "from efficientunet import *\n",
    "def get_axial_segmentation_model(checkpoint):\n",
    "    model = get_efficientunet_b5(out_channels=2, concat_input=True, pretrained=False)\n",
    "    \n",
    "    state = torch.load(checkpoint, map_location=torch.device(device))\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "    \n",
    "seg_model = get_axial_segmentation_model(segmentation_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c3197",
   "metadata": {
    "papermill": {
     "duration": 5.323167,
     "end_time": "2022-10-17T23:12:51.766525",
     "exception": false,
     "start_time": "2022-10-17T23:12:46.443358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from effdet import create_model\n",
    "\n",
    "def get_axial_detection_model(checkpoint, image_size=512):\n",
    "    model = create_model('efficientdetv2_ds' , bench_task='predict' , num_classes=1 , image_size=(image_size, image_size), pretrained=False, max_det_per_image=1)\n",
    "\n",
    "    state = torch.load(checkpoint, map_location=torch.device(device))\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    \n",
    "    model = model.eval()\n",
    "    return model.to(device)\n",
    "\n",
    "det_model = get_axial_detection_model(axial_det_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7108e17",
   "metadata": {
    "papermill": {
     "duration": 0.021497,
     "end_time": "2022-10-17T23:12:51.796708",
     "exception": false,
     "start_time": "2022-10-17T23:12:51.775211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_axial_boundary_from_segmentation(seg, pixel_spacing, throw=100, tol=0.2, max_mm=100):\n",
    "    \"\"\"\n",
    "    seg : H x W\n",
    "    \"\"\"\n",
    "    image_size = seg.shape[0]\n",
    "    min_size = min(image_size, max_mm / pixel_spacing)\n",
    "    \n",
    "    rows, columns = seg.nonzero(as_tuple=True)\n",
    "    rows.sort()\n",
    "    columns.sort()\n",
    "    \n",
    "    throw = min(len(rows) // 2, throw)\n",
    "    \n",
    "    if(len(rows)) == 0:\n",
    "        return torch.tensor([0, 0, image_size, image_size]).to(device)\n",
    "    \n",
    "    xmin, xmax = columns[throw], columns[-throw]\n",
    "    ymin, ymax = rows[throw], rows[-throw]\n",
    "    \n",
    "    w = (xmax - xmin) * (1 + tol)\n",
    "    h = (ymax - ymax) * (1 + tol)\n",
    "    new_size = max(w, h, min_size)\n",
    "    new_size = min(image_size, new_size)\n",
    "    \n",
    "    xcenter, ycenter = (xmax + xmin) / 2, (ymax + ymin) / 2\n",
    "    \n",
    "    xmin = torch.min(torch.tensor(image_size - new_size), xcenter - new_size / 2)\n",
    "    xmin = xmin.clip(min=0)\n",
    "    \n",
    "    ymin = torch.min(torch.tensor(image_size - new_size), ycenter - new_size / 2)\n",
    "    ymin = ymin.clip(min=0)\n",
    "    \n",
    "    return torch.stack([xmin, ymin, xmin + new_size, ymin + new_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093b06b",
   "metadata": {
    "papermill": {
     "duration": 0.015599,
     "end_time": "2022-10-17T23:12:51.819607",
     "exception": false,
     "start_time": "2022-10-17T23:12:51.804008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_seg(x, model, img_size=256):\n",
    "    \"\"\"\n",
    "    return: N x 1 x H x W\n",
    "    \"\"\"\n",
    "    x = TF.resize(x, (img_size, img_size))\n",
    "    logits = model(x)\n",
    "\n",
    "    classification_score, mse_score = logits.sigmoid().chunk(2, dim=1)\n",
    "    classification_pred = classification_score.gt(0.5).float()\n",
    "    pred = (classification_pred * mse_score)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73c7eb",
   "metadata": {
    "papermill": {
     "duration": 0.0161,
     "end_time": "2022-10-17T23:12:51.842926",
     "exception": false,
     "start_time": "2022-10-17T23:12:51.826826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_axial_boundary(segs, pixel_spacings, seg_img_size=256):\n",
    "    boundary_list = []\n",
    "    for i in range(segs.shape[0]):\n",
    "        seg = segs[i, 0, :, :]\n",
    "        \n",
    "        boundary = get_axial_boundary_from_segmentation(seg, pixel_spacings[i], throw=int(100 / 512 * seg_img_size), tol=0.2, max_mm=100 / 512 * seg_img_size)\n",
    "        boundary_list.append(boundary)\n",
    "    boundary_list = torch.stack(boundary_list, axis=0) * (512. / seg_img_size)\n",
    "    return boundary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c9933",
   "metadata": {
    "papermill": {
     "duration": 0.019486,
     "end_time": "2022-10-17T23:12:51.869446",
     "exception": false,
     "start_time": "2022-10-17T23:12:51.849960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_det(x, model):\n",
    "    \n",
    "    bboxes = model(x) # N x 1 x 6\n",
    "    \n",
    "    return bboxes[:, 0, :]\n",
    "    \n",
    "    \n",
    "def crop_resize_images(imgs_tensor, boundary_list, img_size=512):\n",
    "    croped_list = []\n",
    "    for i in range(imgs_tensor.shape[0]):\n",
    "        xmin, ymin, xmax, ymax = boundary_list[i, :]\n",
    "        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "        # print(xmin, ymin, xmax, ymax)\n",
    "        croped = TF.crop(imgs_tensor[i, :, :, :], top=ymin, left=xmin, height=ymax-ymin, width=xmax-xmin)\n",
    "        croped = TF.resize(croped, (img_size, img_size))\n",
    "        croped_list.append(croped)\n",
    "        \n",
    "    return torch.stack(croped_list, 0)\n",
    "\n",
    "def get_original_bbox(bbox, boundary):\n",
    "    scale = 512. / (boundary[:, [2]] - boundary[:, [0]])\n",
    "    \n",
    "    org_bbox = bbox / scale\n",
    "    org_bbox[:, 0] += boundary[:, 0]\n",
    "    org_bbox[:, 1] += boundary[:, 1]\n",
    "    org_bbox[:, 2] += boundary[:, 0]\n",
    "    org_bbox[:, 3] += boundary[:, 1]\n",
    "    \n",
    "    return org_bbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761d857",
   "metadata": {
    "papermill": {
     "duration": 0.014858,
     "end_time": "2022-10-17T23:12:51.891200",
     "exception": false,
     "start_time": "2022-10-17T23:12:51.876342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bbox_class(seg, bbox):\n",
    "    \"\"\"\n",
    "    label 은 0.125 의 단위로, \n",
    "    seg: H x W\n",
    "    bbox: [xmin, ymin, xmax, ymax]\n",
    "    \"\"\"\n",
    "    xmin, ymin, xmax, ymax = bbox.int()\n",
    "    area = seg[ymin:ymax, xmin:xmax]\n",
    "    \n",
    "    # print(area)\n",
    "    result = torch.mean(area[area>0])\n",
    "    result = torch.round(result / 0.125)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f293c",
   "metadata": {
    "papermill": {
     "duration": 0.015036,
     "end_time": "2022-10-17T23:12:51.913209",
     "exception": false,
     "start_time": "2022-10-17T23:12:51.898173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bbox_class_list(seg_list, seg_bboxes):\n",
    "    class_list = []\n",
    "    for i in range(seg_list.shape[0]):\n",
    "        class_index = get_bbox_class(seg_list[i, :, :], seg_bboxes[i, :])\n",
    "        class_list.append(class_index)\n",
    "        \n",
    "    return torch.stack(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafc4c1",
   "metadata": {
    "papermill": {
     "duration": 0.017294,
     "end_time": "2022-10-17T23:12:51.937629",
     "exception": false,
     "start_time": "2022-10-17T23:12:51.920335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_class_score(scores, class_list, eps=1e-2):\n",
    "    result = scores.new_zeros((scores.shape[0], 8)) + eps\n",
    "    class_list = torch.nan_to_num(class_list).long()\n",
    "    result[torch.arange(scores.shape[0]), class_list] = scores\n",
    "    \n",
    "    return result\n",
    "\n",
    "def check_detection_result(det_result, img_size=512., threshold=0.2):\n",
    "    # throw big bboxes\n",
    "    areas = (det_result[:, 2] - det_result[:, 0]) * (det_result[:, 3] - det_result[:, 1]) / (img_size * img_size)\n",
    "    # print(areas)\n",
    "    big_indices = torch.argwhere(areas > threshold)\n",
    "    det_result[big_indices, 4] = 0.\n",
    "    return det_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc7c91",
   "metadata": {
    "papermill": {
     "duration": 0.016415,
     "end_time": "2022-10-17T23:12:51.961355",
     "exception": false,
     "start_time": "2022-10-17T23:12:51.944940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cal_loss(prob, label):\n",
    "    \n",
    "    pos_weight = np.array([14, 2, 2, 2, 2, 2, 2, 2])\n",
    "    neg_weight = np.array([7, 1, 1, 1, 1, 1, 1, 1])\n",
    "    \n",
    "    score = pos_weight * label * np.log(prob) + neg_weight * (1 - label) * np.log(1 - prob)\n",
    "    \n",
    "    weight_total = pos_weight * label + neg_weight * (1 - label)\n",
    "    \n",
    "    return -score.sum(axis=1) / weight_total.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070264d1",
   "metadata": {
    "papermill": {
     "duration": 59.319485,
     "end_time": "2022-10-17T23:13:51.287944",
     "exception": false,
     "start_time": "2022-10-17T23:12:51.968459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "UID = ''\n",
    "test_slices = glob.glob(f'{DATA_DIR}/1.2.826.0.1.3680043.22*/*')\n",
    "test_slices = [re.findall(f'{TRAIN_IMAGES_PATH}/(.*)/(.*).dcm', s)[0] for s in test_slices]\n",
    "df_test_slices = pd.DataFrame(data=test_slices, columns=['StudyInstanceUID', 'Slice']).astype({'Slice': int}).sort_values(['StudyInstanceUID', 'Slice']).reset_index(drop=True)\n",
    "df_test_slices\n",
    "\n",
    "\n",
    "tf = DataTransform()\n",
    "ds = DcmDataSet(df_test_slices, IMAGES_DIR, tf)\n",
    "input, pixel_spacing = ds[0]\n",
    "print(input.shape)\n",
    "print(input.min(), input.max())\n",
    "print(pixel_spacing)\n",
    "batch_size = 16\n",
    "dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=min(os.cpu_count(), batch_size))\n",
    "\n",
    "# plt.imshow(input.permute(1, 2, 0))\n",
    "\n",
    "def predict():\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        \n",
    "        for (x, pixel_spacings) in tqdm(dl):\n",
    "            \n",
    "            # x : N x 3 x 512 x 512\n",
    "            x = x.to(device)\n",
    "            \n",
    "            batch_probs = x.new_zeros((x.shape[0], 8)) + 1e-2\n",
    "            \n",
    "            seg_result = predict_seg(x, seg_model)  # N x 1 x 256 x 256\n",
    "            \n",
    "            active_indices = seg_result.sum(axis=[1, 2, 3]).nonzero().reshape(-1)\n",
    "            if active_indices.numel() == 0:\n",
    "                predictions.append(batch_probs)\n",
    "                continue\n",
    "            \n",
    "            if active_indices.numel() != batch_size:\n",
    "                x = x[active_indices, :, :, :]\n",
    "                seg_result = seg_result[active_indices, :, :, :]\n",
    "                pixel_spacings = pixel_spacings[active_indices]\n",
    "            \n",
    "            \n",
    "            axial_boundary = get_axial_boundary(seg_result, pixel_spacings, seg_img_size=256)  # N x 4\n",
    "            \n",
    "            x = crop_resize_images(x, axial_boundary) # N x 3 x 512 x 512 croped\n",
    "            det_result = predict_det(x, det_model)\n",
    "\n",
    "            bboxes, scores = get_original_bbox(det_result[:, :4], axial_boundary), det_result[:, 4]\n",
    "\n",
    "            class_list = get_bbox_class_list(seg_result[:, 0, :, :], bboxes / 2)\n",
    "\n",
    "            probs = get_class_score(scores, class_list)\n",
    "            \n",
    "            batch_probs[active_indices, :] = probs\n",
    "#             print(probs)\n",
    "            predictions.append(batch_probs)\n",
    "        \n",
    "        return torch.concat(predictions).cpu().numpy()\n",
    "predictions = predict()\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06543192",
   "metadata": {
    "papermill": {
     "duration": 0.045028,
     "end_time": "2022-10-17T23:13:51.345484",
     "exception": false,
     "start_time": "2022-10-17T23:13:51.300456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_effnet_pred = pd.DataFrame(\n",
    "    data=predictions, columns=['patient_overall'] + [f'C{i}' for i in range(1, 8)]\n",
    ")\n",
    "df_test_pred = pd.concat([df_test_slices, df_effnet_pred], axis=1).sort_values(['StudyInstanceUID', 'Slice'])\n",
    "df_patient_pred = df_test_pred.groupby('StudyInstanceUID').apply(lambda df: df.max())\n",
    "df_patient_pred[\"patient_overall\"] = df_patient_pred[[f'C{i}' for i in range(1, 8)]].max(axis=1)\n",
    "df_patient_pred = df_patient_pred[['patient_overall'] + [f'C{i}' for i in range(1, 8)]]\n",
    "df_patient_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50881e1",
   "metadata": {
    "papermill": {
     "duration": 0.019466,
     "end_time": "2022-10-17T23:13:51.376712",
     "exception": false,
     "start_time": "2022-10-17T23:13:51.357246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prob = df_patient_pred.values\n",
    "# label = train_df.loc[df_patient_pred.index].values\n",
    "\n",
    "# losses = cal_loss(prob, label)\n",
    "# print(list(losses))\n",
    "# print(losses)\n",
    "# print(np.mean(losses))\n",
    "# list(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a08b89",
   "metadata": {
    "papermill": {
     "duration": 0.031652,
     "end_time": "2022-10-17T23:13:51.419879",
     "exception": false,
     "start_time": "2022-10-17T23:13:51.388227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../input/rsna-2022-cervical-spine-fracture-detection/test.csv')\n",
    "df_test = pd.read_csv(f'../input/rsna-2022-cervical-spine-fracture-detection/test.csv')\n",
    "\n",
    "if df_test.iloc[0].row_id == '1.2.826.0.1.3680043.10197_C1':\n",
    "    # test_images and test.csv are inconsistent in the dev dataset, fixing labels for the dev run.\n",
    "    df_test = pd.DataFrame({\n",
    "        \"row_id\": ['1.2.826.0.1.3680043.22327_C1', '1.2.826.0.1.3680043.25399_C1', '1.2.826.0.1.3680043.5876_patient_overall'],\n",
    "        \"StudyInstanceUID\": ['1.2.826.0.1.3680043.22327', '1.2.826.0.1.3680043.25399', '1.2.826.0.1.3680043.5876'],\n",
    "        \"prediction_type\": [\"C1\", \"C1\", \"patient_overall\"]}\n",
    "    )\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6c41b",
   "metadata": {
    "papermill": {
     "duration": 0.037254,
     "end_time": "2022-10-17T23:13:51.468836",
     "exception": false,
     "start_time": "2022-10-17T23:13:51.431582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub = df_test.copy()\n",
    "df_sub = df_sub.set_index('StudyInstanceUID').join(df_patient_pred)\n",
    "df_sub['fractured'] = df_sub.apply(lambda r: r[r.prediction_type], axis=1)\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a304f",
   "metadata": {
    "papermill": {
     "duration": 0.023324,
     "end_time": "2022-10-17T23:13:51.504653",
     "exception": false,
     "start_time": "2022-10-17T23:13:51.481329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub[['row_id', 'fractured']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13064fce",
   "metadata": {
    "papermill": {
     "duration": 0.011423,
     "end_time": "2022-10-17T23:13:51.528934",
     "exception": false,
     "start_time": "2022-10-17T23:13:51.517511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f462183",
   "metadata": {
    "papermill": {
     "duration": 0.011779,
     "end_time": "2022-10-17T23:13:51.552423",
     "exception": false,
     "start_time": "2022-10-17T23:13:51.540644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d9aef",
   "metadata": {
    "papermill": {
     "duration": 0.011386,
     "end_time": "2022-10-17T23:13:51.575339",
     "exception": false,
     "start_time": "2022-10-17T23:13:51.563953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 242.404406,
   "end_time": "2022-10-17T23:13:53.977777",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-17T23:09:51.573371",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
