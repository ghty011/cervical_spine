{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import sys\n",
    "\n",
    "effdet_path = \"../third/effdet\"\n",
    "sys.path.append(effdet_path)\n",
    "timm_path = \"../third/timm-pytorch-image-models\"\n",
    "sys.path.append(timm_path)\n",
    "import timm\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from matplotlib import patches\n",
    "import sklearn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "omega_path = \"../third/omegaconf\"\n",
    "sys.path.append(omega_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.models as models\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.Size([2, 16, 128, 128])\n",
      "torch.Size([2, 24, 64, 64])\n",
      "torch.Size([2, 40, 32, 32])\n",
      "torch.Size([2, 112, 16, 16])\n",
      "torch.Size([2, 320, 8, 8])\n",
      "torch.Size([2, 64, 128, 128])\n",
      "torch.Size([2, 64, 64, 64])\n",
      "torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 64, 16, 16])\n",
      "torch.Size([2, 64, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "\n",
    "class EfficientSeg(EfficientDet):\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        print(len(x))\n",
    "        for xi in x:\n",
    "            print(xi.shape)\n",
    "        # print(x)\n",
    "        x = self.fpn(x)\n",
    "\n",
    "        for xi in x:\n",
    "            print(xi.shape)\n",
    "        return x\n",
    "\n",
    "def get_model():\n",
    "    config = get_efficientdet_config('tf_efficientdet_lite0')\n",
    "    config[\"image_size\"] = (256, 256)\n",
    "    config[\"min_level\"] = 1\n",
    "    config[\"max_level\"] = 5\n",
    "    config[\"backbone_indices\"] = (0, 1, 2, 3, 4)\n",
    "    model = EfficientSeg(config, pretrained_backbone=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model().to(device)\n",
    "# model\n",
    "input = torch.randn(2, 3, 256, 256).to(device)\n",
    "y = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.Size([2, 810, 32, 32])\n",
      "torch.Size([2, 810, 2, 2])\n",
      "5\n",
      "torch.Size([2, 36, 32, 32])\n",
      "torch.Size([2, 36, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(y)\n",
    "for iy in y:\n",
    "    print(len(iy))\n",
    "    print(iy[0].shape)\n",
    "    print(iy[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, seg_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.seg_dir = seg_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = self.df.iloc[idx]\n",
    "        UID = df.name\n",
    "\n",
    "        index = int(df[f\"_index\"])\n",
    "\n",
    "        slice_img = Image.open(os.path.join(self.image_dir, UID, f\"{index}.jpeg\"))\n",
    "        label_img = Image.open(os.path.join(self.seg_dir, UID, f\"{index}.png\"))\n",
    "\n",
    "        left, top, right, bottom = df[['left','top','right','bottom']]\n",
    "        slice_img = TF.crop(slice_img, top, left, bottom-top, right-left)\n",
    "        label_img = TF.crop(label_img, top, left, bottom-top, right-left)\n",
    "\n",
    "        # label_img = np.round(np.asarray(label_img, np.uint8) // 32)\n",
    "        if self.transform:\n",
    "            slice_img, label_img = self.transform(slice_img, label_img)\n",
    "\n",
    "        return slice_img, label_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
