{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2148ab38-783d-4c84-8521-f42f3e6e3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_checkpoint=\"axial_segmentation_effseg_163725-epoch-100.pth\"\n",
    "axial_det_checkpoint=\"axial_detection_effdet_134352-epoch-52.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9f1787-2943-4036-8b92-6f98775df0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import sys\n",
    "\n",
    "import pydicom\n",
    "import pylibjpeg\n",
    "\n",
    "effdet_path = \"../third/effdet\"\n",
    "sys.path.append(effdet_path)\n",
    "from effdet import create_model\n",
    "\n",
    "timm_path = \"../third/timm-pytorch-image-models\"\n",
    "sys.path.append(timm_path)\n",
    "import timm\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from matplotlib import patches\n",
    "import sklearn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "omega_path = \"../third/omegaconf\"\n",
    "sys.path.append(omega_path)\n",
    "from omegaconf import OmegaConf\n",
    "import glob\n",
    "import sklearn\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "# from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "\n",
    "# from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "# pos_weight = torch.tensor(pos_weight)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6704ff-0ffe-4073-8fd5-78bd051cd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/root/autodl-tmp/cervical_spine/\"\n",
    "\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'train_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e085bb4-108e-4993-a279-772d389e8a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>prediction_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.10197_C1</td>\n",
       "      <td>1.2.826.0.1.3680043.10197</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.10454_C1</td>\n",
       "      <td>1.2.826.0.1.3680043.10454</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.10690_C1</td>\n",
       "      <td>1.2.826.0.1.3680043.10690</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         row_id           StudyInstanceUID prediction_type\n",
       "0  1.2.826.0.1.3680043.10197_C1  1.2.826.0.1.3680043.10197              C1\n",
       "1  1.2.826.0.1.3680043.10454_C1  1.2.826.0.1.3680043.10454              C1\n",
       "2  1.2.826.0.1.3680043.10690_C1  1.2.826.0.1.3680043.10690              C1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba3177f-b148-4c1b-ba44-168e13bf3ae3",
   "metadata": {},
   "source": [
    "## read dcm file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0edf2b08-0219-47c2-8897-8cf7ac3c7da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_img_to_hu(dcm_ds):\n",
    "    \"\"\"Rescales the image to Hounsfield unit.\n",
    "    \"\"\"\n",
    "    return dcm_ds.pixel_array * dcm_ds.RescaleSlope + dcm_ds.RescaleIntercept\n",
    "\n",
    "\n",
    "def read_dcm(patient_dir, num_instance):\n",
    "    dcm_path = os.path.join(patient_dir, f\"{int(num_instance)}.dcm\")\n",
    "    ds = pydicom.dcmread(dcm_path)\n",
    "    img2d = rescale_img_to_hu(ds)\n",
    "    return normalize_hu(img2d)\n",
    "\n",
    "def read_patient_dcm(patient_dir):\n",
    "    \"\"\"\n",
    "    여기서 이미지를 정상적인 순서로 돌려 놓는다\n",
    "    :param patient_dir:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_slices = len(glob.glob(patient_dir + \"/*.dcm\"))\n",
    "    # print(f\"total slices {num_slices}\")\n",
    "    imgs = np.zeros((num_slices, 512, 512))\n",
    "    image_positions = np.zeros((num_slices, 3))\n",
    "    image_orientations = np.zeros((num_slices, 6))\n",
    "    pixel_spacings = np.zeros((num_slices, 2))\n",
    "    slice_thicknesses = np.zeros((num_slices, 1))\n",
    "\n",
    "    ignore_count = 1\n",
    "    for i in range(num_slices):\n",
    "        dcm_path = os.path.join(patient_dir, f\"{i+ignore_count}.dcm\")\n",
    "        while os.path.exists(dcm_path) == False:\n",
    "            ignore_count += 1\n",
    "            dcm_path = os.path.join(patient_dir, f\"{i+ignore_count}.dcm\")\n",
    "        ds = pydicom.dcmread(dcm_path)\n",
    "\n",
    "        image_positions[i, :] = ds.ImagePositionPatient\n",
    "        image_orientations[i, :] = ds.ImageOrientationPatient\n",
    "        pixel_spacings[i, :] = ds.PixelSpacing\n",
    "        slice_thicknesses[i, :] = ds.SliceThickness\n",
    "\n",
    "        img2d = rescale_img_to_hu(ds)\n",
    "\n",
    "        imgs[i] = img2d\n",
    "\n",
    "    is_flip = False\n",
    "    # check z is in good direction\n",
    "    if image_positions[0, 2] < image_positions[1, 2]:\n",
    "        is_flip = True\n",
    "        # flip image in z direction\n",
    "        imgs = np.flip(imgs, axis=0)\n",
    "        image_positions = np.flip(image_positions, axis=0)\n",
    "        pixel_spacings = np.flip(pixel_spacings, axis=0)\n",
    "        slice_thicknesses = np.flip(slice_thicknesses, axis=0)\n",
    "\n",
    "    aspect = calculate_aspect(image_positions, pixel_spacings)\n",
    "        \n",
    "    return imgs, aspect, pixel_spacings[0, 0]\n",
    "\n",
    "\n",
    "def normalize_hu(data):\n",
    "    # normalize to 0-1\n",
    "    # return (data - data.min()) / data.max()\n",
    "    data = np.clip(data, a_min=-2242, a_max=2242) / 4484 + 0.5\n",
    "    return data\n",
    "\n",
    "def calculate_aspect(image_positions, pixel_spacings):\n",
    "    \"\"\"\n",
    "    calculate z aspect, z 를 몇배로 늘여야 하는가야\n",
    "    :param image_positions:\n",
    "    :param pixel_spacings:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    height = image_positions[0, 2] - image_positions[1, 2]\n",
    "    pixel_spacing = pixel_spacings[0, 0]\n",
    "    aspect = height / pixel_spacing\n",
    "    return aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d07942a7-d40c-48a1-b13f-cde0d4dd48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientunet import *\n",
    "def get_axial_segmentation_model(checkpoint):\n",
    "    model = get_efficientunet_b5(out_channels=2, concat_input=True, pretrained=True)\n",
    "    \n",
    "    state = torch.load(os.path.join(DATA_DIR, 'checkpoint', checkpoint))\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "    \n",
    "seg_model = get_axial_segmentation_model(segmentation_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066c8eba-fef3-4141-aae1-c6730d9c47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet import create_model\n",
    "\n",
    "def get_axial_detection_model(checkpoint, image_size=512):\n",
    "    model = create_model('efficientdetv2_ds' , bench_task='predict' , num_classes=1 , image_size=(image_size, image_size), pretrained=True, max_det_per_image=1)\n",
    "\n",
    "    state = torch.load(os.path.join(DATA_DIR, 'checkpoint',checkpoint))\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "\n",
    "det_model = get_axial_detection_model(axial_det_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d68c789-2abd-44d8-978a-04d9fa3abd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>patient_overall</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>1.2.826.0.1.3680043.10001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>1.2.826.0.1.3680043.10005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>1.2.826.0.1.3680043.10014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>1.2.826.0.1.3680043.10016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>1.2.826.0.1.3680043.10032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7\n",
       "1837  1.2.826.0.1.3680043.10001                0   0   0   0   0   0   0   0\n",
       "823   1.2.826.0.1.3680043.10005                0   0   0   0   0   0   0   0\n",
       "1021  1.2.826.0.1.3680043.10014                0   0   0   0   0   0   0   0\n",
       "667   1.2.826.0.1.3680043.10016                1   0   1   0   0   0   0   0\n",
       "322   1.2.826.0.1.3680043.10032                0   0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv')).sort_values('StudyInstanceUID')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b27033a-d331-4e9b-abe6-ff1c9539b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 398\n",
    "# UID = train_df.iloc[index]['StudyInstanceUID']\n",
    "# print(UID)\n",
    "# label = train_df.iloc[index][['patient_overall', 'C1','C2','C3','C4','C5','C6','C7']].values\n",
    "# print(label)\n",
    "# imgs, aspect, pixel_spacing = read_patient_dcm(os.path.join(IMAGES_DIR, UID))\n",
    "# print(imgs.shape)\n",
    "# print(aspect)\n",
    "# print(pixel_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bb73996-6552-4df4-9054-f3020d3d227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cliped_imgs = np.clip(imgs, a_min=-2242, a_max=2242) / 4484 + 0.5\n",
    "# imgs_tensor = torch.tensor(cliped_imgs, dtype=torch.float)\n",
    "# imgs_tensor = (imgs_tensor - 0.5) * 2.\n",
    "# print(imgs_tensor.shape)\n",
    "# print(imgs_tensor.min(), imgs_tensor.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e77256-2055-423d-a900-9239b9ac1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(imgs_tensor[100, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d7bbc-ba26-4623-ac44-bddd9f838c4a",
   "metadata": {},
   "source": [
    "## get boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f8ec026-a592-4005-81ae-243f9959a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_axial_boundary_from_segmentation(seg, pixel_spacing, throw=100, tol=0.2, max_mm=100):\n",
    "    \"\"\"\n",
    "    seg : H x W\n",
    "    \"\"\"\n",
    "    image_size = seg.shape[0]\n",
    "    min_size = min(image_size, max_mm / pixel_spacing)\n",
    "    \n",
    "    rows, columns = seg.nonzero(as_tuple=True)\n",
    "    rows.sort()\n",
    "    columns.sort()\n",
    "    \n",
    "    throw = min(len(rows) // 2, throw)\n",
    "    \n",
    "    if(len(rows)) == 0:\n",
    "        return torch.tensor([0, 0, image_size, image_size]).to(device)\n",
    "    \n",
    "    xmin, xmax = columns[throw], columns[-throw]\n",
    "    ymin, ymax = rows[throw], rows[-throw]\n",
    "    \n",
    "    w = (xmax - xmin) * (1 + tol)\n",
    "    h = (ymax - ymax) * (1 + tol)\n",
    "    new_size = max(w, h, min_size)\n",
    "    new_size = min(image_size, new_size)\n",
    "    \n",
    "    xcenter, ycenter = (xmax + xmin) / 2, (ymax + ymin) / 2\n",
    "    \n",
    "    xmin = torch.min(torch.tensor(image_size - new_size), xcenter - new_size / 2)\n",
    "    xmin = xmin.clip(min=0)\n",
    "    \n",
    "    ymin = torch.min(torch.tensor(image_size - new_size), ycenter - new_size / 2)\n",
    "    ymin = ymin.clip(min=0)\n",
    "    \n",
    "    return torch.stack([xmin, ymin, xmin + new_size, ymin + new_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2df5fe-74a2-4ebd-be9b-304f9eaa8fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0433ccc6-6a41-4c30-928e-107a92ec5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_seg(x, model):\n",
    "    \"\"\"\n",
    "    return: N x 1 x H x W\n",
    "    \"\"\"\n",
    "    x = x.to(device)\n",
    "    logits = model(x)\n",
    "\n",
    "    classification_score, mse_score = logits.sigmoid().chunk(2, dim=1)\n",
    "    classification_pred = classification_score.gt(0.5).float()\n",
    "    pred = (classification_pred * mse_score)\n",
    "    \n",
    "    return pred, classification_pred.mean(dim=(1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e957e2c6-c642-4d4a-be25-591a083c80af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def seg_patient(imgs_tensor, model, batch_size=64, area_threshold=0.01):\n",
    "    \n",
    "    seg_result = []\n",
    "    areas = []\n",
    "    for i in range(math.ceil(imgs_tensor.shape[0] / batch_size)):\n",
    "        batch_tensor = imgs_tensor[i * batch_size : i * batch_size + batch_size]\n",
    "        batch_tensor = torch.stack((batch_tensor, batch_tensor, batch_tensor), axis=1)\n",
    "        batch_tensor = TF.resize(batch_tensor, (256, 256))\n",
    "        # print(batch_tensor.shape)\n",
    "        \n",
    "        seg, area = predict_seg(batch_tensor, model)\n",
    "        seg_result.append(seg)\n",
    "        areas.append(area)\n",
    "    \n",
    "    \n",
    "    return torch.cat(seg_result, axis=0), torch.cat(areas, axis=0)\n",
    "\n",
    "# seg_result = seg_patient(imgs_tensor, seg_model)\n",
    "# print(seg_result.shape)\n",
    "# plt.imshow(seg_result[seg_result.shape[0] // 2, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3f1d443-2cf0-4119-bcb6-c371f485c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_axial_boundary(segs, pixel_spacing, img_size=256):\n",
    "    boundary_list = []\n",
    "    for i in range(segs.shape[0]):\n",
    "        seg = segs[i, 0, :, :]\n",
    "        # print(seg.shape)\n",
    "        boundary = get_axial_boundary_from_segmentation(seg, pixel_spacing, throw=50, tol=0.2, max_mm=50)\n",
    "#         print(boundary)\n",
    "        boundary_list.append(boundary)\n",
    "    boundary_list = torch.stack(boundary_list, axis=0) * (512. / img_size)\n",
    "    return boundary_list\n",
    "\n",
    "# axial_boundary = get_axial_boundary(seg_result)\n",
    "# print(axial_boundary.shape)\n",
    "\n",
    "# print(axial_boundary[axial_boundary.shape[0] // 2, :])\n",
    "\n",
    "# boundary = axial_boundary[axial_boundary.shape[0] // 2, :] / 2\n",
    "# plt.imshow(seg_result[seg_result.shape[0] // 2, 0, :, :])\n",
    "# plt.axvline(boundary[0])\n",
    "# plt.axvline(boundary[2])\n",
    "# plt.axhline(boundary[1])\n",
    "# plt.axhline(boundary[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194fd269-6de8-40dc-87bf-62877ee8be61",
   "metadata": {},
   "source": [
    "### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cde30be-ae6d-4c55-b391-8a1ae9e2a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_det(x, model):\n",
    "    x = x.to(device)\n",
    "    bboxes = model(x) # N x 1 x 6\n",
    "    \n",
    "    return bboxes[:, 0, :]\n",
    "    \n",
    "    \n",
    "def crop_resize_images(imgs_tensor, boundary_list, img_size=512):\n",
    "    croped_list = []\n",
    "    for i in range(imgs_tensor.shape[0]):\n",
    "        xmin, ymin, xmax, ymax = boundary_list[i, :]\n",
    "        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "        # print(xmin, ymin, xmax, ymax)\n",
    "        croped = TF.crop(imgs_tensor[i, :, :, :], top=ymin, left=xmin, height=ymax-ymin, width=xmax-xmin)\n",
    "        croped = TF.resize(croped, (img_size, img_size))\n",
    "        croped_list.append(croped)\n",
    "        \n",
    "    return torch.stack(croped_list, 0)\n",
    "\n",
    "def get_original_bbox(bbox, boundary):\n",
    "    scale = 512. / (boundary[:, [2]] - boundary[:, [0]])\n",
    "    \n",
    "    org_bbox = bbox / scale\n",
    "    org_bbox[:, 0] += boundary[:, 0]\n",
    "    org_bbox[:, 1] += boundary[:, 1]\n",
    "    org_bbox[:, 2] += boundary[:, 0]\n",
    "    org_bbox[:, 3] += boundary[:, 1]\n",
    "    \n",
    "    return org_bbox\n",
    "\n",
    "def check_detection_result(det_result, img_size=512., threshold=0.2):\n",
    "    # throw big bboxes\n",
    "    areas = (det_result[:, 2] - det_result[:, 0]) * (det_result[:, 3] - det_result[:, 1]) / (img_size * img_size)\n",
    "    # print(areas)\n",
    "    big_indices = torch.argwhere(areas > threshold)\n",
    "    det_result[big_indices, 4] = 0.\n",
    "    return det_result\n",
    "\n",
    "@torch.no_grad()\n",
    "def det_patient(imgs_tensor, model, boundary, batch_size=8):\n",
    "    det_result = []\n",
    "    \n",
    "    imgs_tensor = torch.stack((imgs_tensor[0:-2, :, :], imgs_tensor[1:-1, :, :], imgs_tensor[2:, :, :]), axis=1)\n",
    "   \n",
    "    boundary = boundary[1:-1, :]\n",
    "  \n",
    "    imgs_tensor = crop_resize_images(imgs_tensor, boundary)\n",
    "    \n",
    "    # print(imgs_tensor.shape)\n",
    "    for i in range(math.ceil(imgs_tensor.shape[0] / batch_size)):\n",
    "        batch_tensor = imgs_tensor[i * batch_size : i * batch_size + batch_size]\n",
    "        \n",
    "        pred = predict_det(batch_tensor, model)\n",
    "        det_result.append(pred)\n",
    "    \n",
    "    # det_result.append(torch.tensor([0,0,0,0,0,0]))\n",
    "    det_result = torch.cat(det_result, axis=0)\n",
    "    det_result = check_detection_result(det_result, threshold=0.1)\n",
    "    bboxes, scores = get_original_bbox(det_result[:, :4], boundary), det_result[:, 4]\n",
    "    \n",
    "    return bboxes, scores\n",
    "\n",
    "# bboxes, scores = det_patient(imgs_tensor, det_model, axial_boundary)\n",
    "# print(bboxes.shape)\n",
    "# plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab4ed949-810e-42fd-a80a-10420d1a2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_score_index = scores.argmax()\n",
    "# print(max_score_index)\n",
    "# print(scores.shape)\n",
    "# print(imgs_tensor.shape)\n",
    "# plt.imshow(imgs_tensor[max_score_index+1, :, :])\n",
    "# plt.axvline(bboxes[max_score_index+1, 0])\n",
    "# plt.axvline(bboxes[max_score_index+1, 2])\n",
    "# plt.axhline(bboxes[max_score_index+1, 1])\n",
    "# plt.axhline(bboxes[max_score_index+1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e2ee199-6844-45b5-8f9d-17a014825d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(seg_result[max_score_index + 1, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e65f0-4053-4373-bd1b-c202e192fe49",
   "metadata": {},
   "source": [
    "### get bbox class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a730b84a-d20c-4424-acf0-f5c4cc055493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_class(seg, bbox):\n",
    "    \"\"\"\n",
    "    label 은 0.125 의 단위로, \n",
    "    seg: H x W\n",
    "    bbox: [xmin, ymin, xmax, ymax]\n",
    "    \"\"\"\n",
    "    xmin, ymin, xmax, ymax = bbox.int()\n",
    "    area = seg[ymin:ymax, xmin:xmax]\n",
    "    \n",
    "    # print(area)\n",
    "    result = torch.mean(area[area>0])\n",
    "    result = torch.round(result / 0.125)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# seg_list = seg_result[1:-1, 0, :, :]\n",
    "# seg_bboxes = bboxes / 2\n",
    "# check_index=362\n",
    "# get_bbox_class(seg_list[check_index, :, :], seg_bboxes[check_index, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a942e0b-cd37-4be0-861f-3677a907b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_class_list(seg_list, seg_bboxes):\n",
    "    class_list = []\n",
    "    for i in range(seg_list.shape[0]):\n",
    "        class_index = get_bbox_class(seg_list[i, :, :], seg_bboxes[i, :])\n",
    "        class_list.append(class_index)\n",
    "        \n",
    "    return torch.stack(class_list)\n",
    "\n",
    "# class_list = get_bbox_class_list(seg_result[1:-1, 0, :, :], bboxes / 2)\n",
    "# print(class_list.shape)\n",
    "# print(f\"max {class_list[max_score_index]}\")\n",
    "# plt.plot(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903c602a-a1c4-449f-9bdf-6430ea0f16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(prob, label):\n",
    "    pos_weight = np.array([14, 2, 2, 2, 2, 2, 2, 2])\n",
    "    neg_weight = np.array([7, 1, 1, 1, 1, 1, 1, 1])\n",
    "    \n",
    "    score = pos_weight * label * np.log(prob) + neg_weight * (1 - label) * np.log(1 - prob)\n",
    "    weight_total = pos_weight * label + neg_weight * (1 - label)\n",
    "    \n",
    "    return -score.sum() / weight_total.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e446b61-3258-448b-86f2-3c335c9c4b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(scores, class_list, eps=1e-2):\n",
    "    \"\"\"\n",
    "    scores: N x 5\n",
    "    class_list: N x 5\n",
    "    \"\"\"\n",
    "    scores = scores.reshape(-1)\n",
    "    class_list = class_list.reshape(-1)\n",
    "    \n",
    "    prob = np.zeros(8) + eps\n",
    "    # prob[0] = scores.max()\n",
    "    \n",
    "    for i in range(len(scores)):\n",
    "        score = scores[i]\n",
    "        \n",
    "        if torch.isnan(class_list[i]):\n",
    "            continue\n",
    "        \n",
    "        class_index = int(class_list[i])\n",
    "        prob[class_index] = min(1-eps, max(prob[class_index], score))\n",
    "    \n",
    "    prob[0] = prob.max()\n",
    "    \n",
    "    return prob\n",
    "\n",
    "# prob = get_prob(scores, class_list)\n",
    "# print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "593d3ef9-40de-49d7-be3e-792196e34437",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_patient(UID, return_prob=True, plot=False):\n",
    "    imgs, aspect, pixel_spacing = read_patient_dcm(os.path.join(IMAGES_DIR, UID))\n",
    "\n",
    "    imgs = np.clip(imgs, a_min=-2242, a_max=2242) / 4484 + 0.5\n",
    "    imgs_tensor = torch.tensor(imgs, dtype=torch.float)\n",
    "    \n",
    "    del imgs\n",
    "    \n",
    "    imgs_tensor = (imgs_tensor - 0.5) * 2.\n",
    "    \n",
    "    seg_result, area_result = seg_patient(imgs_tensor, seg_model)\n",
    "\n",
    "    area_threshold = 1e-3\n",
    "    valid_slices = torch.argwhere(area_result > area_threshold)\n",
    "    slice_start = valid_slices[0]\n",
    "    slice_end = valid_slices[-1]\n",
    "\n",
    "    \n",
    "    seg_result=seg_result[slice_start:slice_end+1, :, :, :]\n",
    "    \n",
    "    axial_boundary = get_axial_boundary(seg_result, pixel_spacing)\n",
    "    \n",
    "\n",
    "    imgs_tensor = imgs_tensor[slice_start:slice_end+1, :, :]\n",
    "    bboxes, scores = det_patient(imgs_tensor, det_model, axial_boundary)\n",
    "    \n",
    "    if plot is True:\n",
    "        print(imgs_tensor.shape, bboxes.shape, scores.shape, seg_result.shape)\n",
    "        max_slice = scores.argmax()\n",
    "        max_bbox = bboxes[max_slice].cpu()\n",
    "        plt.imshow(imgs_tensor[max_slice, :, :].cpu())\n",
    "        plt.axvline(max_bbox[0])\n",
    "        plt.axvline(max_bbox[2])\n",
    "        plt.axhline(max_bbox[1])\n",
    "        plt.axhline(max_bbox[3])\n",
    "    \n",
    "    class_list = get_bbox_class_list(seg_result[1:-1, 0, :, :], bboxes / 2)\n",
    "    prob = get_prob(scores, class_list)\n",
    "    \n",
    "    if return_prob:\n",
    "        return prob\n",
    "    else:\n",
    "        return {\n",
    "            f'{UID}_patient_overall': prob[0],\n",
    "            f'{UID}_C1': prob[1],\n",
    "            f'{UID}_C2': prob[2],\n",
    "            f'{UID}_C3': prob[3],\n",
    "            f'{UID}_C4': prob[4],\n",
    "            f'{UID}_C5': prob[5],\n",
    "            f'{UID}_C6': prob[6],\n",
    "            f'{UID}_C7': prob[7]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1dda57-94e0-4111-b4ae-2a178fa905f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.826.0.1.3680043.19487\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = 628\n",
    "UID = train_df.iloc[index]['StudyInstanceUID']\n",
    "print(UID)\n",
    "label = train_df.iloc[index][['patient_overall', 'C1','C2','C3','C4','C5','C6','C7']].values\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "prob = infer_patient(UID, plot=True)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"time\" , end_time - start_time)\n",
    "\n",
    "print(prob)\n",
    "print(label)\n",
    "loss = cal_loss(prob, label)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2561f3e-cc04-4232-90fc-8870bc67c71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "058d7ad4-e6df-48e2-a3aa-0902c8523fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:07<03:47,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31514627 0.01       0.31514627 0.01451689 0.07741152 0.05994357\n",
      " 0.04126728 0.08951533]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "600 1.2.826.0.1.3680043.19159 loss 0.2379558651514751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:18<04:30,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74741822 0.13900633 0.17769071 0.01       0.01       0.01\n",
      " 0.2166559  0.74741822]\n",
      "[1 0 0 0 0 0 0 1]\n",
      "601 1.2.826.0.1.3680043.19165 loss 0.23989668580892484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:25<03:47,  8.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34408286 0.09891494 0.01       0.03127407 0.34408286 0.02060283\n",
      " 0.01074466 0.03306874]\n",
      "[1 1 0 0 0 0 0 0]\n",
      "602 1.2.826.0.1.3680043.19182 loss 0.9132728301132919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:35<03:53,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99       0.01273708 0.01455453 0.01       0.01       0.01221132\n",
      " 0.01340551 0.99      ]\n",
      "[1 0 0 0 0 0 0 1]\n",
      "603 1.2.826.0.1.3680043.19185 loss 0.010644049525014453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:53<05:04, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77335405 0.02341917 0.03250308 0.01       0.01670659 0.07652367\n",
      " 0.77335405 0.0320382 ]\n",
      "[1 0 0 0 0 0 1 0]\n",
      "604 1.2.826.0.1.3680043.19194 loss 0.19582293510355797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [01:01<04:19, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88476229 0.01       0.06013347 0.05993611 0.88476229 0.06088804\n",
      " 0.01       0.0555192 ]\n",
      "[1 0 0 0 0 1 1 0]\n",
      "605 1.2.826.0.1.3680043.1920 loss 0.8205929831429748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [01:16<04:39, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08341543 0.0143363  0.03046794 0.05444024 0.01327473 0.01057327\n",
      " 0.05795517 0.08341543]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "606 1.2.826.0.1.3680043.19201 loss 0.06299022502587003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [01:24<04:01, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06083947 0.0341849  0.04946635 0.02285674 0.01       0.03875624\n",
      " 0.03814276 0.06083947]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "607 1.2.826.0.1.3680043.19238 loss 0.04994673080591736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [01:31<03:20,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44455326 0.01       0.02124824 0.14519458 0.01       0.01\n",
      " 0.44455326 0.03081846]\n",
      "[1 0 1 0 0 0 0 0]\n",
      "608 1.2.826.0.1.3680043.19242 loss 0.9026762171245647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [01:40<03:07,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66219825 0.15577172 0.22260128 0.02457987 0.09183913 0.0124559\n",
      " 0.10623298 0.66219825]\n",
      "[1 0 0 0 0 0 0 1]\n",
      "609 1.2.826.0.1.3680043.19258 loss 0.3301020051302995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [01:47<02:44,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2194953  0.03208593 0.05560268 0.2194953  0.01       0.01590591\n",
      " 0.07095525 0.02322289]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "610 1.2.826.0.1.3680043.19266 loss 0.1568225457880959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [02:04<03:24, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0845563  0.02447422 0.02917543 0.01       0.0845563  0.01981866\n",
      " 0.01275763 0.01929872]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "611 1.2.826.0.1.3680043.19283 loss 0.05882533086904317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [02:20<03:37, 12.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27598491 0.27598491 0.05598766 0.01341496 0.01       0.01\n",
      " 0.01       0.01      ]\n",
      "[1 0 1 0 0 0 0 0]\n",
      "612 1.2.826.0.1.3680043.19298 loss 1.0984375077574964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [02:36<03:39, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92414933 0.01060327 0.01       0.01       0.92414933 0.02349745\n",
      " 0.01442754 0.57521802]\n",
      "[1 0 0 0 1 0 0 0]\n",
      "613 1.2.826.0.1.3680043.19304 loss 0.09942527212881043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [02:47<03:12, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47694185 0.01       0.04582896 0.47694185 0.01       0.02678142\n",
      " 0.08006795 0.01945419]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "614 1.2.826.0.1.3680043.1931 loss 0.38441157935575265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [02:53<02:31, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34412137 0.04348218 0.01       0.01       0.04992831 0.29280096\n",
      " 0.03882232 0.34412137]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "615 1.2.826.0.1.3680043.19333 loss 0.27686068828278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [03:06<02:27, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54558587 0.02138371 0.01912445 0.01       0.01       0.04532678\n",
      " 0.01       0.54558587]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "616 1.2.826.0.1.3680043.19339 loss 0.45910236208945604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [03:17<02:15, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0357272  0.01876572 0.01718717 0.01       0.01       0.01\n",
      " 0.0357272  0.02548813]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "617 1.2.826.0.1.3680043.19343 loss 0.027378461114817947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [03:26<01:56, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87379503 0.04407585 0.01       0.01838702 0.02301975 0.01310121\n",
      " 0.87379503 0.01901452]\n",
      "[1 0 0 0 0 0 1 0]\n",
      "618 1.2.826.0.1.3680043.19352 loss 0.10399593765393633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [03:38<01:49, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08169358 0.08169358 0.01       0.01       0.01312044 0.01708158\n",
      " 0.07350623 0.01255997]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "619 1.2.826.0.1.3680043.19360 loss 0.05866555816118245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [03:46<01:30, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02999294 0.01       0.01       0.01       0.01       0.0104276\n",
      " 0.02999294 0.01      ]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "620 1.2.826.0.1.3680043.19369 loss 0.021739247470673014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [03:54<01:16,  9.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99       0.01       0.04494466 0.99       0.99       0.99\n",
      " 0.99       0.99      ]\n",
      "[1 0 0 1 1 1 1 1]\n",
      "621 1.2.826.0.1.3680043.19381 loss 0.011432476449861884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [04:11<01:21, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99       0.03073288 0.99       0.04261813 0.01       0.01284999\n",
      " 0.08282913 0.03359315]\n",
      "[1 0 1 0 0 0 0 0]\n",
      "622 1.2.826.0.1.3680043.19388 loss 0.017235854552547947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [04:23<01:11, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13902429 0.04008119 0.02262454 0.01       0.01162656 0.13902429\n",
      " 0.02213975 0.04393984]\n",
      "[1 0 0 1 0 0 0 0]\n",
      "623 1.2.826.0.1.3680043.19397 loss 1.6875604907782817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [04:30<00:51, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98994583 0.78860015 0.10156567 0.01       0.05060165 0.01681643\n",
      " 0.10175625 0.98994583]\n",
      "[1 0 0 0 0 0 0 1]\n",
      "624 1.2.826.0.1.3680043.19400 loss 0.09131985660683282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [04:38<00:38,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17477441 0.01       0.01496993 0.01       0.01       0.01\n",
      " 0.17477441 0.01      ]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "625 1.2.826.0.1.3680043.19401 loss 0.11443733422369647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [04:57<00:37, 12.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1022552  0.01       0.01       0.02421187 0.1022552  0.05449566\n",
      " 0.01087543 0.01      ]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "626 1.2.826.0.1.3680043.19410 loss 0.07032771847224341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [05:05<00:22, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38736823 0.06505144 0.38736823 0.01667458 0.10794641 0.01\n",
      " 0.01       0.03032446]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "627 1.2.826.0.1.3680043.19439 loss 0.29779516109355064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [05:14<00:10, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81836838 0.24605472 0.01       0.07086204 0.81836838 0.06977753\n",
      " 0.32003173 0.02816657]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "628 1.2.826.0.1.3680043.19487 loss 1.0356280611773265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [05:21<00:00, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08195743 0.01545361 0.02413123 0.01697075 0.08195743 0.03440809\n",
      " 0.02261562 0.01      ]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "629 1.2.826.0.1.3680043.19517 loss 0.05779639676499192\n",
      "0.3297699455907756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for index in tqdm(range(600, 630)):\n",
    "\n",
    "    UID = train_df.iloc[index]['StudyInstanceUID']\n",
    "    # print(UID)\n",
    "    label = train_df.iloc[index][['patient_overall', 'C1','C2','C3','C4','C5','C6','C7']].values\n",
    "    \n",
    "    prob = infer_patient(UID)\n",
    "    loss = cal_loss(prob, label)\n",
    "    print(prob)\n",
    "    print(label)\n",
    "    print(f'{index} {UID} loss {loss}')\n",
    "    losses.append(loss)\n",
    "    \n",
    "print(np.mean(losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3bf9b23-693a-4f7b-b0ee-0c7778571060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2379558651514751,\n",
       " 0.23989668580892484,\n",
       " 0.9132728301132919,\n",
       " 0.010644049525014453,\n",
       " 0.19582293510355797,\n",
       " 0.8205929831429748,\n",
       " 0.06299022502587003,\n",
       " 0.04994673080591736,\n",
       " 0.9026762171245647,\n",
       " 0.3301020051302995,\n",
       " 0.1568225457880959,\n",
       " 0.05882533086904317,\n",
       " 1.0984375077574964,\n",
       " 0.09942527212881043,\n",
       " 0.38441157935575265,\n",
       " 0.27686068828278,\n",
       " 0.45910236208945604,\n",
       " 0.027378461114817947,\n",
       " 0.10399593765393633,\n",
       " 0.05866555816118245,\n",
       " 0.021739247470673014,\n",
       " 0.011432476449861884,\n",
       " 0.017235854552547947,\n",
       " 1.6875604907782817,\n",
       " 0.09131985660683282,\n",
       " 0.11443733422369647,\n",
       " 0.07032771847224341,\n",
       " 0.29779516109355064,\n",
       " 1.0356280611773265,\n",
       " 0.05779639676499192]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b802763-5fd5-4c49-8e66-7a1d1b89ef57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
