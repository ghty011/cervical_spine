{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9f1787-2943-4036-8b92-6f98775df0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import sys\n",
    "\n",
    "import pydicom\n",
    "import pylibjpeg\n",
    "\n",
    "effdet_path = \"../third/effdet\"\n",
    "sys.path.append(effdet_path)\n",
    "from effdet import create_model\n",
    "\n",
    "timm_path = \"../third/timm-pytorch-image-models\"\n",
    "sys.path.append(timm_path)\n",
    "import timm\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from matplotlib import patches\n",
    "import sklearn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "omega_path = \"../third/omegaconf\"\n",
    "sys.path.append(omega_path)\n",
    "from omegaconf import OmegaConf\n",
    "import glob\n",
    "import sklearn\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "# from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "\n",
    "# from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "# pos_weight = torch.tensor(pos_weight)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6704ff-0ffe-4073-8fd5-78bd051cd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/root/autodl-tmp/cervical_spine/\"\n",
    "\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'train_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e085bb4-108e-4993-a279-772d389e8a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>prediction_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.10197_C1</td>\n",
       "      <td>1.2.826.0.1.3680043.10197</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.10454_C1</td>\n",
       "      <td>1.2.826.0.1.3680043.10454</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.10690_C1</td>\n",
       "      <td>1.2.826.0.1.3680043.10690</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         row_id           StudyInstanceUID prediction_type\n",
       "0  1.2.826.0.1.3680043.10197_C1  1.2.826.0.1.3680043.10197              C1\n",
       "1  1.2.826.0.1.3680043.10454_C1  1.2.826.0.1.3680043.10454              C1\n",
       "2  1.2.826.0.1.3680043.10690_C1  1.2.826.0.1.3680043.10690              C1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba3177f-b148-4c1b-ba44-168e13bf3ae3",
   "metadata": {},
   "source": [
    "## read dcm file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0edf2b08-0219-47c2-8897-8cf7ac3c7da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_img_to_hu(dcm_ds):\n",
    "    \"\"\"Rescales the image to Hounsfield unit.\n",
    "    \"\"\"\n",
    "    return dcm_ds.pixel_array * dcm_ds.RescaleSlope + dcm_ds.RescaleIntercept\n",
    "\n",
    "\n",
    "def read_dcm(patient_dir, num_instance):\n",
    "    dcm_path = os.path.join(patient_dir, f\"{int(num_instance)}.dcm\")\n",
    "    ds = pydicom.dcmread(dcm_path)\n",
    "    img2d = rescale_img_to_hu(ds)\n",
    "    return normalize_hu(img2d)\n",
    "\n",
    "def read_patient_dcm(patient_dir):\n",
    "    \"\"\"\n",
    "    여기서 이미지를 정상적인 순서로 돌려 놓는다\n",
    "    :param patient_dir:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_slices = len(glob.glob(patient_dir + \"/*.dcm\"))\n",
    "    print(f\"total slices {num_slices}\")\n",
    "    imgs = np.zeros((num_slices, 512, 512))\n",
    "    image_positions = np.zeros((num_slices, 3))\n",
    "    image_orientations = np.zeros((num_slices, 6))\n",
    "    pixel_spacings = np.zeros((num_slices, 2))\n",
    "    slice_thicknesses = np.zeros((num_slices, 1))\n",
    "\n",
    "    ignore_count = 1\n",
    "    for i in range(num_slices):\n",
    "        dcm_path = os.path.join(patient_dir, f\"{i+ignore_count}.dcm\")\n",
    "        while os.path.exists(dcm_path) == False:\n",
    "            ignore_count += 1\n",
    "            dcm_path = os.path.join(patient_dir, f\"{i+ignore_count}.dcm\")\n",
    "        ds = pydicom.dcmread(dcm_path)\n",
    "\n",
    "        image_positions[i, :] = ds.ImagePositionPatient\n",
    "        image_orientations[i, :] = ds.ImageOrientationPatient\n",
    "        pixel_spacings[i, :] = ds.PixelSpacing\n",
    "        slice_thicknesses[i, :] = ds.SliceThickness\n",
    "\n",
    "        img2d = rescale_img_to_hu(ds)\n",
    "\n",
    "        imgs[i] = img2d\n",
    "\n",
    "    is_flip = False\n",
    "    # check z is in good direction\n",
    "    if image_positions[0, 2] < image_positions[1, 2]:\n",
    "        is_flip = True\n",
    "        # flip image in z direction\n",
    "        imgs = np.flip(imgs, axis=0)\n",
    "        image_positions = np.flip(image_positions, axis=0)\n",
    "        pixel_spacings = np.flip(pixel_spacings, axis=0)\n",
    "        slice_thicknesses = np.flip(slice_thicknesses, axis=0)\n",
    "\n",
    "    aspect = calculate_aspect(image_positions, pixel_spacings)\n",
    "        \n",
    "    return imgs, aspect\n",
    "\n",
    "\n",
    "def normalize_hu(data):\n",
    "    # normalize to 0-1\n",
    "    # return (data - data.min()) / data.max()\n",
    "    data = np.clip(data, a_min=-2242, a_max=2242) / 4484 + 0.5\n",
    "    return data\n",
    "\n",
    "def calculate_aspect(image_positions, pixel_spacings):\n",
    "    \"\"\"\n",
    "    calculate z aspect, z 를 몇배로 늘여야 하는가야\n",
    "    :param image_positions:\n",
    "    :param pixel_spacings:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    height = image_positions[0, 2] - image_positions[1, 2]\n",
    "    pixel_spacing = pixel_spacings[0, 0]\n",
    "    aspect = height / pixel_spacing\n",
    "    return aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d7bbc-ba26-4623-ac44-bddd9f838c4a",
   "metadata": {},
   "source": [
    "## get boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04500c5a-2461-4cb7-9471-704979f8fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientunet import *\n",
    "def get_axial_segmentation_model(checkpoint):\n",
    "    model = get_efficientunet_b5(out_channels=2, concat_input=True, pretrained=True)\n",
    "    \n",
    "    state = torch.load(os.path.join(DATA_DIR, 'checkpoint', checkpoint))\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "    \n",
    "model = get_axial_segmentation_model(segmentation_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ec026-a592-4005-81ae-243f9959a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_axial_boundary(seg, pixel_spacing, throw=100, tol=0.2, max_mm=100):\n",
    "    image_size = seg.shape[0]\n",
    "    min_size = min(image_size, max_mm / pixel_spacing)\n",
    "    \n",
    "    rows, columns = seg.nonzero()\n",
    "    rows.sort()\n",
    "    columns.sort()\n",
    "    \n",
    "    throw = min(len(rows) // 2, throw)\n",
    "    \n",
    "    if(len(rows)) == 0:\n",
    "        return 0, 0, image_size, image_size\n",
    "    \n",
    "    xmin, xmax = columns[throw], columns[-throw]\n",
    "    ymin, ymax = rows[throw], rows[-throw]\n",
    "    \n",
    "    w = (xmax - xmin) * (1 + tol)\n",
    "    h = (ymax - ymax) * (1 + tol)\n",
    "    new_size = max(w, h, min_size)\n",
    "    new_size = min(image_size, new_size)\n",
    "    \n",
    "    xcenter, ycenter = (xmax + xmin) / 2, (ymax + ymin) / 2\n",
    "    \n",
    "    xmin = min(image_size - new_size, xcenter - new_size / 2)\n",
    "    xmin = max(0, xmin)\n",
    "    \n",
    "    ymin = min(image_size - new_size, ycenter - new_size / 2)\n",
    "    ymin = max(0, ymin)\n",
    "    \n",
    "    return xmin, ymin, xmin + new_size, ymin + new_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de92a9b-e267-4fb9-9b19-16492c34b63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2df5fe-74a2-4ebd-be9b-304f9eaa8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxialSegmentationTransform(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "\n",
    "        \n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(image_size, image_size),\n",
    "            ToTensorV2(p=1),\n",
    "        ])\n",
    "\n",
    "        self.normalize = T.Normalize(255 * 0.5, 255 * 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        augmented = self.transform(image=np.asarray(x))\n",
    "        x= augmented['image']\n",
    "\n",
    "        x = self.normalize(x.float())\n",
    "\n",
    "        return torch.cat((x, x, x), dim=0)\n",
    "    \n",
    "seg_transform = AxialSegmentationTransform(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433ccc6-6a41-4c30-928e-107a92ec5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_seg(x, model):\n",
    "    x = x.to(device)\n",
    "    logits = model(x)\n",
    "\n",
    "    classification_score, mse_score = logits.sigmoid().chunk(2, dim=1)\n",
    "    classification_pred = classification_score.gt(0.5).float()\n",
    "    pred = (classification_pred * mse_score).cpu().numpy()\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957e2c6-c642-4d4a-be25-591a083c80af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seg_patient(UID):\n",
    "    for i in tqdm(range(280)):\n",
    "        axial_index = i\n",
    "        img = Image.open(os.path.join(IMAGES_DIR, UID, f'{i}.jpeg'))\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        label = predict(x, model)\n",
    "        label = np.round(label / 0.125) * 0.125\n",
    "        label = label.squeeze()\n",
    "        class_label = np.mean(label[label > 0])\n",
    "        class_label_list.append(class_label)\n",
    "        xmin, ymin, xmax, ymax = get_axial_boundary(label, pixel_spacing, throw=50, tol=0.2, max_mm=50)\n",
    "        axs[i//10, i % 10].imshow(label, cmap='nipy_spectral')\n",
    "        axs[i//10, i % 10].axvline(xmin)\n",
    "        axs[i//10, i % 10].axvline(xmax)\n",
    "        axs[i//10, i % 10].axhline(ymin)\n",
    "        axs[i//10, i % 10].axhline(ymax)\n",
    "        axs[i//10, i % 10].set_title(class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a82efd69-ec1e-44a6-9323-d553cd9d1297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>patient_overall</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>1.2.826.0.1.3680043.10001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>1.2.826.0.1.3680043.10005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>1.2.826.0.1.3680043.10014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>1.2.826.0.1.3680043.10016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>1.2.826.0.1.3680043.10032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7\n",
       "1837  1.2.826.0.1.3680043.10001                0   0   0   0   0   0   0   0\n",
       "823   1.2.826.0.1.3680043.10005                0   0   0   0   0   0   0   0\n",
       "1021  1.2.826.0.1.3680043.10014                0   0   0   0   0   0   0   0\n",
       "667   1.2.826.0.1.3680043.10016                1   0   1   0   0   0   0   0\n",
       "322   1.2.826.0.1.3680043.10032                0   0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv')).sort_values('StudyInstanceUID')\n",
    "train_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
