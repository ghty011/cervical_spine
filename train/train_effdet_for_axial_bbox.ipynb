{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bboxes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "batch_size = 2\n",
    "wd = 1e-5\n",
    "pos_weight = 2\n",
    "image_size = 256\n",
    "# backbone=\"segmentation\"\n",
    "backbone=\"none\"\n",
    "vertical_type = \"axial\"\n",
    "train_portion = 0.5\n",
    "milestones = [50, 100, 150, 200]\n",
    "model_name = \"effdet\"\n",
    "\n",
    "slice_range=5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import sys\n",
    "\n",
    "effdet_path = \"../third/effdet\"\n",
    "sys.path.append(effdet_path)\n",
    "timm_path = \"../third/timm-pytorch-image-models\"\n",
    "sys.path.append(timm_path)\n",
    "import timm\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from matplotlib import patches\n",
    "import sklearn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "omega_path = \"../third/omegaconf\"\n",
    "sys.path.append(omega_path)\n",
    "from omegaconf import OmegaConf\n",
    "import glob\n",
    "import sklearn\n",
    "import math\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "IMAGE_SIZE = 512\n",
    "\n",
    "\n",
    "pos_weight = torch.tensor(pos_weight)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_DIR = \"/Volumes/SSD970/\"\n",
    "# DATA_DIR = \"/root/autodl-tmp/cervical_spine/\"\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'train_axial_images')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, 'bbox_clean.csv'))\n",
    "print(len(df))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_rectangle_edges_from_pascal_bbox(bbox):\n",
    "    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n",
    "\n",
    "    # bottom_left = (xmin_top_left, ymax_bottom_right)\n",
    "    width = xmax_bottom_right - xmin_top_left\n",
    "    height = ymin_top_left - ymax_bottom_right\n",
    "\n",
    "    return xmin_top_left, ymax_bottom_right, width, height\n",
    "\n",
    "def draw_pascal_voc_bboxes(\n",
    "        plot_ax,\n",
    "        bboxes,\n",
    "        get_rectangle_corners_fn=get_rectangle_edges_from_pascal_bbox,\n",
    "):\n",
    "    for bbox in bboxes:\n",
    "        x, y, width, height = get_rectangle_corners_fn(bbox)\n",
    "        # print(bbox)\n",
    "        # x, y, width, height = bbox\n",
    "\n",
    "\n",
    "        rect_1 = patches.Rectangle(\n",
    "            (x, y),\n",
    "            width,\n",
    "            height,\n",
    "            linewidth=4,\n",
    "            edgecolor=\"black\",\n",
    "            fill=False,\n",
    "        )\n",
    "        rect_2 = patches.Rectangle(\n",
    "            (x, y),\n",
    "            width,\n",
    "            height,\n",
    "            linewidth=2,\n",
    "            edgecolor=\"white\",\n",
    "            fill=False,\n",
    "        )\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        plot_ax.add_patch(rect_1)\n",
    "        plot_ax.add_patch(rect_2)\n",
    "\n",
    "def show_image(\n",
    "        image, bboxes=None, draw_bboxes_fn=draw_pascal_voc_bboxes, figsize=(10, 10)\n",
    "):\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    if bboxes is not None:\n",
    "        draw_bboxes_fn(ax, bboxes)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# image = Image.open(os.path.join(IMAGES_DIR, '1.2.826.0.1.3680043.10051', '142.jpeg'))\n",
    "# bboxes = [list(df.iloc[10][['x', 'y','width','height']])]\n",
    "# print(bboxes)\n",
    "# show_image(image, bboxes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Adaptor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FractureDatasetAdaptor(Dataset):\n",
    "    def __init__(self, images_dir_path, df):\n",
    "        self.image_dir = images_dir_path\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_image_and_labels_by_idx(self, idx):\n",
    "        s = self.df.iloc[idx]\n",
    "        # img = Image.open(os.path.join(self.images_dir_path, s['StudyInstanceUID'], f\"{s['axial_index']}.jpeg\"))\n",
    "\n",
    "        UID = s['StudyInstanceUID']\n",
    "        slice = s['axial_index']\n",
    "        r = Image.open(os.path.join(self.image_dir, UID, f\"{slice-1}.jpeg\"))\n",
    "        g = Image.open(os.path.join(self.image_dir, UID, f\"{slice-0}.jpeg\"))\n",
    "        b = Image.open(os.path.join(self.image_dir, UID, f\"{slice+1}.jpeg\"))\n",
    "        img = Image.merge('RGB', (r, g, b))\n",
    "\n",
    "        pascal_bboxes = [[s.x, s.y, s.x + s.width, s.y + s.height]]\n",
    "        class_labels = np.ones(1)\n",
    "\n",
    "        return img, pascal_bboxes, class_labels, idx\n",
    "\n",
    "    def show_image(self, index):\n",
    "        image, bboxes, class_labels, image_id = self.get_image_and_labels_by_idx(index)\n",
    "        print(f\"image_id: {image_id}\")\n",
    "        show_image(image, bboxes)\n",
    "        print(class_labels)\n",
    "\n",
    "da = FractureDatasetAdaptor(IMAGES_DIR, df)\n",
    "# da.show_image(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_train_transforms(target_img_size=512):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Resize(height=target_img_size, width=target_img_size, p=1),\n",
    "            # A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            A.Normalize(0.5, 0.5),\n",
    "            ToTensorV2(p=1),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"pascal_voc\", min_area=0, min_visibility=0, label_fields=[\"labels\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transforms(target_img_size=512):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=target_img_size, width=target_img_size, p=1),\n",
    "            # A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            A.Normalize(0.5, 0.5),\n",
    "            ToTensorV2(p=1),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"pascal_voc\", min_area=0, min_visibility=0, label_fields=[\"labels\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "class EfficientDetDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, dataset_adaptor, transforms=get_valid_transforms()\n",
    "    ):\n",
    "        self.ds = dataset_adaptor\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        (\n",
    "            image,\n",
    "            pascal_bboxes,\n",
    "            class_labels,\n",
    "            image_id,\n",
    "        ) = self.ds.get_image_and_labels_by_idx(index)\n",
    "\n",
    "        sample = {\n",
    "            \"image\": np.array(image, dtype=np.float32),\n",
    "            \"bboxes\": pascal_bboxes,\n",
    "            \"labels\": class_labels,\n",
    "        }\n",
    "\n",
    "        sample = self.transforms(**sample)\n",
    "        sample[\"bboxes\"] = np.array(sample[\"bboxes\"])\n",
    "        image = sample[\"image\"]\n",
    "        # pascal_bboxes = sample[\"bboxes\"]\n",
    "        labels = sample[\"labels\"]\n",
    "\n",
    "        _, new_h, new_w = image.shape\n",
    "        sample[\"bboxes\"][:, [0, 1, 2, 3]] = sample[\"bboxes\"][\n",
    "                                            :, [1, 0, 3, 2]\n",
    "                                            ]  # convert to yxyx\n",
    "\n",
    "        target = {\n",
    "            \"bboxes\": torch.as_tensor(sample[\"bboxes\"], dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels),\n",
    "            \"image_id\": torch.tensor([image_id]),\n",
    "            \"img_size\": (new_h, new_w),\n",
    "            \"img_scale\": torch.tensor([1.]),\n",
    "            # \"img_scale\": [1., 1.],\n",
    "        }\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "train_dataset = EfficientDetDataset(da, transforms=get_train_transforms())\n",
    "img, ann, idx = train_dataset[0]\n",
    "print(ann)\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "ax.imshow(img[0, :, :])\n",
    "# print(ann['bboxes'])\n",
    "# draw_pascal_voc_bboxes(ax, ann['bboxes'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, targets, image_ids = tuple(zip(*batch))\n",
    "    images = torch.stack(images)\n",
    "    images = images.float()\n",
    "\n",
    "    boxes = [target[\"bboxes\"].float() for target in targets]\n",
    "    labels = [target[\"labels\"].float() for target in targets]\n",
    "    img_size = torch.tensor([target[\"img_size\"] for target in targets]).float()\n",
    "    img_scale = torch.tensor([target[\"img_scale\"] for target in targets]).float()\n",
    "\n",
    "    annotations = {\n",
    "        \"bbox\": boxes,\n",
    "        \"cls\": labels,\n",
    "        \"img_size\": img_size,\n",
    "        \"img_scale\": img_scale,\n",
    "    }\n",
    "\n",
    "    return images, annotations, targets, image_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # shuffle=True,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    # num_workers=min(os.cpu_count(), 16),\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, annotations, _, image_ids = next(iter(train_loader))\n",
    "print(annotations)\n",
    "print(images.shape)\n",
    "# print(_)\n",
    "# print(image_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from effdet import create_model\n",
    "model = create_model('tf_efficientdet_lite0' , bench_task='train' , num_classes= 1 , image_size=(IMAGE_SIZE,IMAGE_SIZE),bench_labeler=True,pretrained=True)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bbox': [tensor([[ 0.3011,  1.4055,  0.1306, -0.9147],\n",
      "        [-0.9309,  1.1566, -0.0894, -0.5522]]), tensor([[-0.3299, -0.7674, -0.3774,  0.4820],\n",
      "        [ 1.7670,  0.0377, -0.8770, -1.5318],\n",
      "        [ 0.1933, -0.3323, -0.3887, -0.5790]])], 'cls': [tensor([1., 1.]), tensor([1., 1., 1.])]}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/3l/2d1zfhx90z18_kvtkwwz9xtw0000gn/T/ipykernel_32656/3440181326.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m }\n\u001B[1;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mannotations\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mannotations\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/StudyCode/kaggle/cervical_spine/train/../third/kaggle-effdet/effdet/bench.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, target)\u001B[0m\n\u001B[1;32m    139\u001B[0m             \u001B[0mnum_positives\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'label_num_positives'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             cls_targets, box_targets, num_positives = self.anchor_labeler.batch_label_anchors(\n\u001B[0m\u001B[1;32m    142\u001B[0m                 target['bbox'], target['cls'])\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/StudyCode/kaggle/cervical_spine/train/../third/kaggle-effdet/effdet/anchors.py\u001B[0m in \u001B[0;36mbatch_label_anchors\u001B[0;34m(self, gt_boxes, gt_classes, filter_valid)\u001B[0m\n\u001B[1;32m    396\u001B[0m                     cls_targets[count:count + steps].view([feat_size[0], feat_size[1], -1]))\n\u001B[1;32m    397\u001B[0m                 box_targets_out[level_idx].append(\n\u001B[0;32m--> 398\u001B[0;31m                     box_targets[count:count + steps].view([feat_size[0], feat_size[1], -1]))\n\u001B[0m\u001B[1;32m    399\u001B[0m                 \u001B[0mcount\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0msteps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    400\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mlast_sample\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "images = torch.randn((2, 3, 512, 512)).to(device)\n",
    "annotations = {\n",
    "    'bbox' : [torch.randn((2, 4)).to(device), torch.randn((3, 4)).to(device)],\n",
    "    'cls' : [torch.ones(2).to(device), torch.ones(3).to(device)]\n",
    "}\n",
    "print(annotations)\n",
    "model(images, annotations)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
