{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "wd = 5e-5\n",
    "pos_weight = 20\n",
    "mse_weight = 100  # relative to classification error\n",
    "image_size = 256\n",
    "backbone=\"resnet50\"\n",
    "vertical_type = \"sagittal\"\n",
    "train_portion = 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb_entity='longyi'\n",
    "model_name = \"detection\"\n",
    "wandb.init(project=\"cervical-spine\", entity=wandb_entity, config={\n",
    "    \"model\":model_name,\n",
    "    \"batch_size\":batch_size,\n",
    "    \"lr\" : lr,\n",
    "    \"wd\" : wd,\n",
    "    \"pos_weight\" : pos_weight,\n",
    "    \"backbone\" : backbone,\n",
    "    \"image_size\" : image_size,\n",
    "})\n",
    "wandb.run.name = f'{vertical_type}_{model_name}_c2_center_' + datetime.now().strftime(\"%H%M%S\")\n",
    "wandb.run.name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pydicom\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image, ImageOps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, CenterCrop\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.models as models\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# DATA_DIR = \"/media/longyi/SSD9701/\"\n",
    "DATA_DIR = \"/Volumes/SSD970/\"\n",
    "# DATA_DIR = \"/root/autodl-tmp/cervical_spine/\"\n",
    "TRAIN_IMAGES_DIR = os.path.join(DATA_DIR, \"train_images\")\n",
    "\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, f\"train_{vertical_type}_images_jpeg95\")\n",
    "LABEL_DIR = os.path.join(DATA_DIR, f\"segmentation_{vertical_type}_labels\")\n",
    "MASK_DIR = os.path.join(DATA_DIR, \"train_sagittal_labels_jpeg95\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n"
     ]
    },
    {
     "data": {
      "text/plain": "                           C2  sagittal_center_slice  label_scale  left  top  \\\nUID                                                                            \n1.2.826.0.1.3680043.10001   0                    281           32    49   98   \n1.2.826.0.1.3680043.10005   0                    255           32    22    0   \n1.2.826.0.1.3680043.10014   0                    261           32    28  148   \n1.2.826.0.1.3680043.10016   1                    278           32    37   98   \n1.2.826.0.1.3680043.10032   0                    250           32    28  125   \n\n                           right  bottom  \nUID                                       \n1.2.826.0.1.3680043.10001    377     590  \n1.2.826.0.1.3680043.10005    366     291  \n1.2.826.0.1.3680043.10014    480     882  \n1.2.826.0.1.3680043.10016    396     601  \n1.2.826.0.1.3680043.10032    413     584  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C2</th>\n      <th>sagittal_center_slice</th>\n      <th>label_scale</th>\n      <th>left</th>\n      <th>top</th>\n      <th>right</th>\n      <th>bottom</th>\n    </tr>\n    <tr>\n      <th>UID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1.2.826.0.1.3680043.10001</th>\n      <td>0</td>\n      <td>281</td>\n      <td>32</td>\n      <td>49</td>\n      <td>98</td>\n      <td>377</td>\n      <td>590</td>\n    </tr>\n    <tr>\n      <th>1.2.826.0.1.3680043.10005</th>\n      <td>0</td>\n      <td>255</td>\n      <td>32</td>\n      <td>22</td>\n      <td>0</td>\n      <td>366</td>\n      <td>291</td>\n    </tr>\n    <tr>\n      <th>1.2.826.0.1.3680043.10014</th>\n      <td>0</td>\n      <td>261</td>\n      <td>32</td>\n      <td>28</td>\n      <td>148</td>\n      <td>480</td>\n      <td>882</td>\n    </tr>\n    <tr>\n      <th>1.2.826.0.1.3680043.10016</th>\n      <td>1</td>\n      <td>278</td>\n      <td>32</td>\n      <td>37</td>\n      <td>98</td>\n      <td>396</td>\n      <td>601</td>\n    </tr>\n    <tr>\n      <th>1.2.826.0.1.3680043.10032</th>\n      <td>0</td>\n      <td>250</td>\n      <td>32</td>\n      <td>28</td>\n      <td>125</td>\n      <td>413</td>\n      <td>584</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, 'meta_sagittal_c2_center.csv')).set_index(\"UID\")\n",
    "print(len(df))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/SSD970/train_sagittal_images_jpeg95/1.2.826.0.1.3680043.10001/281.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/3l/2d1zfhx90z18_kvtkwwz9xtw0000gn/T/ipykernel_32209/3379966864.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSagittalDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mIMAGES_DIR\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mMASK_DIR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 31\u001B[0;31m \u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     32\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msubplots\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfigsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m12\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m12\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/3l/2d1zfhx90z18_kvtkwwz9xtw0000gn/T/ipykernel_32209/3379966864.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0mUID\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m         \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mImage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimage_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mUID\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf\"{s.sagittal_center_slice}.jpeg\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m         \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mleft\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbottom\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mright\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mleft\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/PIL/Image.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   2973\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2974\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mfilename\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2975\u001B[0;31m         \u001B[0mfp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuiltins\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2976\u001B[0m         \u001B[0mexclusive_fp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2977\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Volumes/SSD970/train_sagittal_images_jpeg95/1.2.826.0.1.3680043.10001/281.jpeg'"
     ]
    }
   ],
   "source": [
    "class SagittalDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, mask_dir, transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.df.iloc[idx]\n",
    "        UID = s.name\n",
    "        img = Image.open(os.path.join(self.image_dir, UID, f\"{s.sagittal_center_slice}.jpeg\"))\n",
    "        img = TF.crop(img, s.top, s.left, s.bottom - s.top, s.right - s.left)\n",
    "\n",
    "        mask = Image.open(os.path.join(self.mask_dir, UID, f\"{s.sagittal_center_slice}.png\"))\n",
    "        mask = np.asarray(mask)\n",
    "\n",
    "        label = s.C2\n",
    "\n",
    "        if self.transform:\n",
    "            img, mask, label = self.transform(img, mask, label)\n",
    "\n",
    "        return img, mask, label\n",
    "\n",
    "\n",
    "dataset = SagittalDataset(df, IMAGES_DIR, MASK_DIR)\n",
    "img, mask, label = dataset[0]\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(12, 12))\n",
    "print(label)\n",
    "axs[0].imshow(img, cmap='bone')\n",
    "\n",
    "axs[1].imshow(mask, cmap=\"nipy_spectral\")\n",
    "# axs[1].axhline(126)\n",
    "# axs[1].axvline(125)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class DataTransform(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(image_size),\n",
    "            T.RandomAutocontrast(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(0.5, 0.5)\n",
    "        ])\n",
    "\n",
    "        # self.mask_transform = T.Normalize(0, 32)\n",
    "\n",
    "        self.label_transform = T.ToTensor()\n",
    "\n",
    "    def forward(self, x, mask, label):\n",
    "        x = TF.center_crop(x, max(x.width, x.height))\n",
    "        x = self.transform(x)\n",
    "\n",
    "        # print(mask)\n",
    "        mask = torch.tensor(mask, dtype=torch.float) / 32.\n",
    "\n",
    "        label = torch.tensor(label).long()\n",
    "        return x, mask, label\n",
    "\n",
    "\n",
    "transform = DataTransform(image_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def split_dataset(df, train_portion=0.5):\n",
    "    df = shuffle(df)\n",
    "    train_end_index = int(len(df) * train_portion)\n",
    "    train_df = df.iloc[:train_end_index]\n",
    "    val_df = df.iloc[train_end_index:]\n",
    "    return train_df, val_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005 1006\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = split_dataset(df, train_portion)\n",
    "print(len(train_df), len(val_df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/SSD970/train_sagittal_images_jpeg95/1.2.826.0.1.3680043.10005/255.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/3l/2d1zfhx90z18_kvtkwwz9xtw0000gn/T/ipykernel_32209/715690396.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSagittalDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mIMAGES_DIR\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mMASK_DIR\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/3l/2d1zfhx90z18_kvtkwwz9xtw0000gn/T/ipykernel_32209/3379966864.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0mUID\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m         \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mImage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimage_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mUID\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf\"{s.sagittal_center_slice}.jpeg\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m         \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mleft\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbottom\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mright\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mleft\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/PIL/Image.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   2973\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2974\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mfilename\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2975\u001B[0;31m         \u001B[0mfp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuiltins\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2976\u001B[0m         \u001B[0mexclusive_fp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2977\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Volumes/SSD970/train_sagittal_images_jpeg95/1.2.826.0.1.3680043.10005/255.jpeg'"
     ]
    }
   ],
   "source": [
    "dataset = SagittalDataset(df, IMAGES_DIR, MASK_DIR, transform=transform)\n",
    "img, mask, label = dataset[1]\n",
    "print(label)\n",
    "mask.max()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_backbone():\n",
    "    backbone = models.resnet50(pretrained=True)\n",
    "    conv1_weight = backbone.conv1.weight\n",
    "    conv1_weight = conv1_weight.mean(dim=1).unsqueeze(1)\n",
    "\n",
    "    backbone.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=1, padding='same', bias=False)\n",
    "    backbone.conv1.weight = nn.Parameter(conv1_weight, requires_grad=True)\n",
    "\n",
    "    return nn.ModuleList([\n",
    "        nn.Sequential(\n",
    "            backbone.conv1,\n",
    "            backbone.bn1,\n",
    "            backbone.relu,\n",
    "            backbone.maxpool),\n",
    "        nn.Sequential(\n",
    "            backbone.layer1,\n",
    "            backbone.layer2,\n",
    "        ),\n",
    "        backbone.layer3,\n",
    "        backbone.layer4\n",
    "    ]), [1, 64, 512, 1024, 2048]\n",
    "\n",
    "# backbone, channels = get_backbone()\n",
    "# print(backbone)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, backbone, channels, spine=2, deep=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.deep = deep\n",
    "        self.channels = channels\n",
    "        self.spine = spine\n",
    "        self.dw = nn.Parameter(torch.tensor(20.0, dtype=torch.float), requires_grad=False)\n",
    "        # self.register_buffer('mf', (torch.tensor(spine) * 0.125).reshape(1, 1, 1))\n",
    "        self.init_layers()\n",
    "\n",
    "    def init_layers(self):\n",
    "        self.parallel_modules_1 = self.make_parallel_modules()\n",
    "        self.parallel_modules_2 = self.make_parallel_modules()\n",
    "        self.downsampling_modules = self.make_downsampling_modules()\n",
    "        self.mask_modules = self.make_mask_modules()\n",
    "        self.classification_modules = self.make_classification_modules()\n",
    "\n",
    "    def make_parallel_modules(self):\n",
    "        parallel_modules = nn.ModuleList()\n",
    "\n",
    "        for channel in self.channels:\n",
    "            module = nn.Conv2d(channel, channel, kernel_size=1, padding='same')\n",
    "            parallel_modules.append(module)\n",
    "\n",
    "        return parallel_modules\n",
    "\n",
    "    def make_mask_modules(self):\n",
    "        mask_modules = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.deep):\n",
    "            module = nn.Sequential(\n",
    "                # nn.Conv2d(self.channels[i], self.channels[i+1], kernel_size=3, stride=2, padding=1),\n",
    "                # nn.Sigmoid()\n",
    "                nn.MaxPool2d(3, stride=2, padding=1)\n",
    "            )\n",
    "            mask_modules.append(module)\n",
    "\n",
    "        return mask_modules\n",
    "\n",
    "    def make_downsampling_modules(self):\n",
    "        downsampling_modules = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.deep):\n",
    "            module = nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(self.channels[i], self.channels[i], kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(self.channels[i], self.channels[i + 1], kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "            )\n",
    "            downsampling_modules.append(module)\n",
    "\n",
    "        return downsampling_modules\n",
    "\n",
    "    def make_classification_modules(self):\n",
    "        return nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.channels[-1], self.channels[-1], kernel_size=3, padding='same'),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.channels[-1] * 16 * 16, 1)\n",
    "        )\n",
    "\n",
    "    def forward_recursive(self, x, modules):\n",
    "        result = []\n",
    "        out = x\n",
    "        for module in modules:\n",
    "            out = module(out)\n",
    "            result.append(out)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def forward_parallel(self, inputs, modules):\n",
    "        result = []\n",
    "        for input, module in zip(inputs, modules):\n",
    "            out = module(input)\n",
    "            result.append(out)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def apply_mask(self, inputs, masks):\n",
    "        result = []\n",
    "\n",
    "        for input, mask in zip(inputs, masks):\n",
    "            out = input * mask\n",
    "            result.append(out)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def repeat_mask(self, mask):\n",
    "        N, H, W = mask.shape\n",
    "\n",
    "        mask[mask != (0.125 * self.spine)] = -torch.inf\n",
    "        # mask[mask == 0] = -torch.inf\n",
    "        #\n",
    "        mask  = mask.unsqueeze(1) ** 2\n",
    "        # mask = (mask.unsqueeze(1) - self.mf) ** 2\n",
    "        mask = torch.exp(-self.dw * mask)  # N, 7, H, W\n",
    "        # mask = mask.reshape(-1, 1, H, W) # N, 1, H, W\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward_downsampling(self, features, modules):\n",
    "        out = features[0]\n",
    "        for i, module in enumerate(modules):\n",
    "            out = module(out) + features[i + 1]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        backbone_features = self.forward_recursive(x, self.backbone)\n",
    "        # check_list_nan(backbone_features, \"backbone_features\")\n",
    "\n",
    "        mask = self.repeat_mask(mask)  # 14, 1, 256, 256\n",
    "        mask_features = self.forward_recursive(mask, self.mask_modules)\n",
    "\n",
    "        # check_list_nan(mask_features, \"mask_features\")\n",
    "\n",
    "        parallel_features_1 = self.forward_parallel([x] + backbone_features, self.parallel_modules_1)\n",
    "        # 여기서 뻥튀기를 시킨다.\n",
    "        # parallel_features_1 = [feature.repeat_interleave(self.mf.shape[0], dim=0) for feature in parallel_features_1]\n",
    "\n",
    "        # check_list_nan(parallel_features_1)\n",
    "\n",
    "        masked_features = self.apply_mask(parallel_features_1, [mask] + mask_features)\n",
    "\n",
    "        # check_list_nan(masked_features)\n",
    "        out = self.forward_parallel(masked_features, self.parallel_modules_2)\n",
    "\n",
    "        # check_list_nan(out)\n",
    "        out = self.forward_downsampling(out, self.downsampling_modules)\n",
    "        out = self.classification_modules(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# model = ClassificationModel(backbone, channels).to(device)\n",
    "#\n",
    "# total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(total_params)\n",
    "\n",
    "# input = torch.randn(2, 1, 256, 256).to(device)\n",
    "# mask = torch.randn(2, 256, 256).to(device)\n",
    "# logits = model(input, mask)\n",
    "# logits.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=min(16, batch_size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_fn(logits, y, pos_weight=torch.tensor(1)):\n",
    "    labels = F.one_hot(y, num_classes=7).reshape(-1, 1).float()\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, labels, pos_weight=pos_weight)\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def denormalize_img(x):\n",
    "    img = x.detach().cpu().numpy()\n",
    "    img = (img * 0.5) + 0.5\n",
    "    img = img.transpose(0, 2, 3, 1)\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# backbone, channels = get_backbone()\n",
    "backbone, channels = get_seg_backbone()\n",
    "model = ClassificationModel(backbone, channels).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 10, 15, 20], gamma=0.5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pos_weight = torch.tensor(pos_weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(e, model, dataloader):\n",
    "    model.train()\n",
    "    train_iter = tqdm(dataloader)\n",
    "    losses = []\n",
    "    epoch_iteration = len(dataloader)\n",
    "\n",
    "    for i, (x, mask, y) in enumerate(train_iter):\n",
    "        x = x.to(device)\n",
    "        mask = mask.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(device == 'cuda'):\n",
    "            logits = model(x, mask)\n",
    "            loss = loss_fn(logits, y, pos_weight=pos_weight)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_iter.set_description(f\"t {e} loss {loss.item():.4f}\")\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if wandb.run is not None:\n",
    "            lr_logs = {f\"last_lr_{i}\": float(v) for i, v in enumerate(scheduler.get_last_lr())}\n",
    "            wandb.log({\n",
    "                'train_loss': loss.item(),\n",
    "                'epoch': e,\n",
    "                'train_iteration': i + e * epoch_iteration,\n",
    "                **lr_logs,\n",
    "            })\n",
    "\n",
    "        # if i % 10 == 0:\n",
    "        #     with torch.no_grad():\n",
    "        #         infer_bad_sample(wandb_log=True)\n",
    "        #     model.train()\n",
    "\n",
    "    return np.mean(losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(e, model, dataloader):\n",
    "    model.eval()\n",
    "    eval_iter = tqdm(dataloader)\n",
    "    losses = []\n",
    "    epoch_iteration = len(dataloader)\n",
    "\n",
    "    for i, (x, mask, y) in enumerate(eval_iter):\n",
    "        x = x.to(device)\n",
    "        mask = mask.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(device == 'cuda'):\n",
    "            logits = model(x, mask)\n",
    "            loss = loss_fn(logits, y, pos_weight=pos_weight)\n",
    "            pred = loss.ge(0.5).float()\n",
    "            acc = (pred == y).mean()\n",
    "\n",
    "        eval_iter.set_description(f\"e {e} loss {loss.item():.4f} ecc {acc.item():.4f}\")\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                'eval_loss': loss.item(),\n",
    "                'eval_acc' : acc.item(),\n",
    "                'epoch': e,\n",
    "                'eval_iteration': i + e * epoch_iteration,\n",
    "            })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epoch = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    train_loss = train_one_epoch(epoch, model, train_loader)\n",
    "    epoch += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"model\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"scheduler\": scheduler.state_dict(),\n",
    "    \"epoch\": epoch,\n",
    "}\n",
    "torch.save(state, f'checkpoint/{wandb.run.name}-epoch-{epoch}.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
