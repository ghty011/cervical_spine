{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f29b5f-0431-47a1-936d-b3023b778c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm \n",
    "import random\n",
    "from typing import Optional, Union, List, Tuple\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "from segmentation_models_pytorch.base import (\n",
    "    SegmentationModel,\n",
    "    SegmentationHead,\n",
    "    ClassificationHead,\n",
    ")\n",
    "from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ad3b76-bfb7-4a06-86a8-939582f41061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0560eccf-0939-4a78-bd76-18949f88f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = smp.encoders.TimmUniversalEncoder(\"efficientnetv2_s\", in_channels=1, depth=5, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae01db5-d939-43ae-bc16-d3632572e442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n",
      "torch.Size([1, 24, 32, 32])\n",
      "torch.Size([1, 48, 16, 16])\n",
      "torch.Size([1, 64, 8, 8])\n",
      "torch.Size([1, 160, 4, 4])\n",
      "torch.Size([1, 256, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 여기서 반토막 짜리 들어간다. 그래서 64 64 이다. \n",
    "x = torch.randn(1, 1, 64, 64)\n",
    "out = encoder(x)\n",
    "for feature in out:\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df0cce25-983e-499f-b7e0-a77000d91936",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adv_inception_v3',\n",
       " 'bat_resnext26ts',\n",
       " 'botnet26t_256',\n",
       " 'botnet50ts_256',\n",
       " 'cait_m36_384',\n",
       " 'cait_m48_448',\n",
       " 'cait_s24_224',\n",
       " 'cait_s24_384',\n",
       " 'cait_s36_384',\n",
       " 'cait_xs24_384',\n",
       " 'cait_xxs24_224',\n",
       " 'cait_xxs24_384',\n",
       " 'cait_xxs36_224',\n",
       " 'cait_xxs36_384',\n",
       " 'coat_lite_mini',\n",
       " 'coat_lite_small',\n",
       " 'coat_lite_tiny',\n",
       " 'coat_mini',\n",
       " 'coat_tiny',\n",
       " 'convit_base',\n",
       " 'convit_small',\n",
       " 'convit_tiny',\n",
       " 'cspdarknet53',\n",
       " 'cspdarknet53_iabn',\n",
       " 'cspresnet50',\n",
       " 'cspresnet50d',\n",
       " 'cspresnet50w',\n",
       " 'cspresnext50',\n",
       " 'cspresnext50_iabn',\n",
       " 'darknet53',\n",
       " 'deit_base_distilled_patch16_224',\n",
       " 'deit_base_distilled_patch16_384',\n",
       " 'deit_base_patch16_224',\n",
       " 'deit_base_patch16_384',\n",
       " 'deit_small_distilled_patch16_224',\n",
       " 'deit_small_patch16_224',\n",
       " 'deit_tiny_distilled_patch16_224',\n",
       " 'deit_tiny_patch16_224',\n",
       " 'densenet121',\n",
       " 'densenet121d',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'densenet264',\n",
       " 'densenet264d_iabn',\n",
       " 'densenetblur121d',\n",
       " 'dla34',\n",
       " 'dla46_c',\n",
       " 'dla46x_c',\n",
       " 'dla60',\n",
       " 'dla60_res2net',\n",
       " 'dla60_res2next',\n",
       " 'dla60x',\n",
       " 'dla60x_c',\n",
       " 'dla102',\n",
       " 'dla102x',\n",
       " 'dla102x2',\n",
       " 'dla169',\n",
       " 'dm_nfnet_f0',\n",
       " 'dm_nfnet_f1',\n",
       " 'dm_nfnet_f2',\n",
       " 'dm_nfnet_f3',\n",
       " 'dm_nfnet_f4',\n",
       " 'dm_nfnet_f5',\n",
       " 'dm_nfnet_f6',\n",
       " 'dpn68',\n",
       " 'dpn68b',\n",
       " 'dpn92',\n",
       " 'dpn98',\n",
       " 'dpn107',\n",
       " 'dpn131',\n",
       " 'eca_botnext26ts_256',\n",
       " 'eca_efficientnet_b0',\n",
       " 'eca_halonext26ts',\n",
       " 'eca_lambda_resnext26ts',\n",
       " 'eca_nfnet_l0',\n",
       " 'eca_nfnet_l1',\n",
       " 'eca_nfnet_l2',\n",
       " 'eca_nfnet_l3',\n",
       " 'eca_swinnext26ts_256',\n",
       " 'eca_vovnet39b',\n",
       " 'ecaresnet26t',\n",
       " 'ecaresnet50d',\n",
       " 'ecaresnet50d_pruned',\n",
       " 'ecaresnet50t',\n",
       " 'ecaresnet101d',\n",
       " 'ecaresnet101d_pruned',\n",
       " 'ecaresnet200d',\n",
       " 'ecaresnet269d',\n",
       " 'ecaresnetlight',\n",
       " 'ecaresnext26t_32x4d',\n",
       " 'ecaresnext50t_32x4d',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b1_pruned',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b2_pruned',\n",
       " 'efficientnet_b2a',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b3_pruned',\n",
       " 'efficientnet_b3a',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_b8',\n",
       " 'efficientnet_cc_b0_4e',\n",
       " 'efficientnet_cc_b0_8e',\n",
       " 'efficientnet_cc_b1_8e',\n",
       " 'efficientnet_el',\n",
       " 'efficientnet_el_pruned',\n",
       " 'efficientnet_em',\n",
       " 'efficientnet_es',\n",
       " 'efficientnet_es_pruned',\n",
       " 'efficientnet_l2',\n",
       " 'efficientnet_lite0',\n",
       " 'efficientnet_lite1',\n",
       " 'efficientnet_lite2',\n",
       " 'efficientnet_lite3',\n",
       " 'efficientnet_lite4',\n",
       " 'efficientnetv2_l',\n",
       " 'efficientnetv2_m',\n",
       " 'efficientnetv2_rw_m',\n",
       " 'efficientnetv2_rw_s',\n",
       " 'efficientnetv2_s',\n",
       " 'ens_adv_inception_resnet_v2',\n",
       " 'ese_vovnet19b_dw',\n",
       " 'ese_vovnet19b_slim',\n",
       " 'ese_vovnet19b_slim_dw',\n",
       " 'ese_vovnet39b',\n",
       " 'ese_vovnet39b_evos',\n",
       " 'ese_vovnet57b',\n",
       " 'ese_vovnet99b',\n",
       " 'ese_vovnet99b_iabn',\n",
       " 'fbnetc_100',\n",
       " 'fbnetv3_b',\n",
       " 'fbnetv3_d',\n",
       " 'fbnetv3_g',\n",
       " 'gc_efficientnet_b0',\n",
       " 'gcresnet50t',\n",
       " 'gcresnext26ts',\n",
       " 'geresnet50t',\n",
       " 'gernet_l',\n",
       " 'gernet_m',\n",
       " 'gernet_s',\n",
       " 'ghostnet_050',\n",
       " 'ghostnet_100',\n",
       " 'ghostnet_130',\n",
       " 'gluon_inception_v3',\n",
       " 'gluon_resnet18_v1b',\n",
       " 'gluon_resnet34_v1b',\n",
       " 'gluon_resnet50_v1b',\n",
       " 'gluon_resnet50_v1c',\n",
       " 'gluon_resnet50_v1d',\n",
       " 'gluon_resnet50_v1s',\n",
       " 'gluon_resnet101_v1b',\n",
       " 'gluon_resnet101_v1c',\n",
       " 'gluon_resnet101_v1d',\n",
       " 'gluon_resnet101_v1s',\n",
       " 'gluon_resnet152_v1b',\n",
       " 'gluon_resnet152_v1c',\n",
       " 'gluon_resnet152_v1d',\n",
       " 'gluon_resnet152_v1s',\n",
       " 'gluon_resnext50_32x4d',\n",
       " 'gluon_resnext101_32x4d',\n",
       " 'gluon_resnext101_64x4d',\n",
       " 'gluon_senet154',\n",
       " 'gluon_seresnext50_32x4d',\n",
       " 'gluon_seresnext101_32x4d',\n",
       " 'gluon_seresnext101_64x4d',\n",
       " 'gluon_xception65',\n",
       " 'gmixer_12_224',\n",
       " 'gmixer_24_224',\n",
       " 'gmlp_b16_224',\n",
       " 'gmlp_s16_224',\n",
       " 'gmlp_ti16_224',\n",
       " 'halonet26t',\n",
       " 'halonet50ts',\n",
       " 'halonet_h1',\n",
       " 'halonet_h1_c4c5',\n",
       " 'hardcorenas_a',\n",
       " 'hardcorenas_b',\n",
       " 'hardcorenas_c',\n",
       " 'hardcorenas_d',\n",
       " 'hardcorenas_e',\n",
       " 'hardcorenas_f',\n",
       " 'hrnet_w18',\n",
       " 'hrnet_w18_small',\n",
       " 'hrnet_w18_small_v2',\n",
       " 'hrnet_w30',\n",
       " 'hrnet_w32',\n",
       " 'hrnet_w40',\n",
       " 'hrnet_w44',\n",
       " 'hrnet_w48',\n",
       " 'hrnet_w64',\n",
       " 'ig_resnext101_32x8d',\n",
       " 'ig_resnext101_32x16d',\n",
       " 'ig_resnext101_32x32d',\n",
       " 'ig_resnext101_32x48d',\n",
       " 'inception_resnet_v2',\n",
       " 'inception_v3',\n",
       " 'inception_v4',\n",
       " 'lambda_resnet26t',\n",
       " 'lambda_resnet50t',\n",
       " 'legacy_senet154',\n",
       " 'legacy_seresnet18',\n",
       " 'legacy_seresnet34',\n",
       " 'legacy_seresnet50',\n",
       " 'legacy_seresnet101',\n",
       " 'legacy_seresnet152',\n",
       " 'legacy_seresnext26_32x4d',\n",
       " 'legacy_seresnext50_32x4d',\n",
       " 'legacy_seresnext101_32x4d',\n",
       " 'levit_128',\n",
       " 'levit_128s',\n",
       " 'levit_192',\n",
       " 'levit_256',\n",
       " 'levit_384',\n",
       " 'mixer_b16_224',\n",
       " 'mixer_b16_224_in21k',\n",
       " 'mixer_b16_224_miil',\n",
       " 'mixer_b16_224_miil_in21k',\n",
       " 'mixer_b32_224',\n",
       " 'mixer_l16_224',\n",
       " 'mixer_l16_224_in21k',\n",
       " 'mixer_l32_224',\n",
       " 'mixer_s16_224',\n",
       " 'mixer_s32_224',\n",
       " 'mixnet_l',\n",
       " 'mixnet_m',\n",
       " 'mixnet_s',\n",
       " 'mixnet_xl',\n",
       " 'mixnet_xxl',\n",
       " 'mnasnet_050',\n",
       " 'mnasnet_075',\n",
       " 'mnasnet_100',\n",
       " 'mnasnet_140',\n",
       " 'mnasnet_a1',\n",
       " 'mnasnet_b1',\n",
       " 'mnasnet_small',\n",
       " 'mobilenetv2_100',\n",
       " 'mobilenetv2_110d',\n",
       " 'mobilenetv2_120d',\n",
       " 'mobilenetv2_140',\n",
       " 'mobilenetv3_large_075',\n",
       " 'mobilenetv3_large_100',\n",
       " 'mobilenetv3_large_100_miil',\n",
       " 'mobilenetv3_large_100_miil_in21k',\n",
       " 'mobilenetv3_rw',\n",
       " 'mobilenetv3_small_075',\n",
       " 'mobilenetv3_small_100',\n",
       " 'nasnetalarge',\n",
       " 'nf_ecaresnet26',\n",
       " 'nf_ecaresnet50',\n",
       " 'nf_ecaresnet101',\n",
       " 'nf_regnet_b0',\n",
       " 'nf_regnet_b1',\n",
       " 'nf_regnet_b2',\n",
       " 'nf_regnet_b3',\n",
       " 'nf_regnet_b4',\n",
       " 'nf_regnet_b5',\n",
       " 'nf_resnet26',\n",
       " 'nf_resnet50',\n",
       " 'nf_resnet101',\n",
       " 'nf_seresnet26',\n",
       " 'nf_seresnet50',\n",
       " 'nf_seresnet101',\n",
       " 'nfnet_f0',\n",
       " 'nfnet_f0s',\n",
       " 'nfnet_f1',\n",
       " 'nfnet_f1s',\n",
       " 'nfnet_f2',\n",
       " 'nfnet_f2s',\n",
       " 'nfnet_f3',\n",
       " 'nfnet_f3s',\n",
       " 'nfnet_f4',\n",
       " 'nfnet_f4s',\n",
       " 'nfnet_f5',\n",
       " 'nfnet_f5s',\n",
       " 'nfnet_f6',\n",
       " 'nfnet_f6s',\n",
       " 'nfnet_f7',\n",
       " 'nfnet_f7s',\n",
       " 'nfnet_l0',\n",
       " 'pit_b_224',\n",
       " 'pit_b_distilled_224',\n",
       " 'pit_s_224',\n",
       " 'pit_s_distilled_224',\n",
       " 'pit_ti_224',\n",
       " 'pit_ti_distilled_224',\n",
       " 'pit_xs_224',\n",
       " 'pit_xs_distilled_224',\n",
       " 'pnasnet5large',\n",
       " 'rednet26t',\n",
       " 'rednet50ts',\n",
       " 'regnetx_002',\n",
       " 'regnetx_004',\n",
       " 'regnetx_006',\n",
       " 'regnetx_008',\n",
       " 'regnetx_016',\n",
       " 'regnetx_032',\n",
       " 'regnetx_040',\n",
       " 'regnetx_064',\n",
       " 'regnetx_080',\n",
       " 'regnetx_120',\n",
       " 'regnetx_160',\n",
       " 'regnetx_320',\n",
       " 'regnety_002',\n",
       " 'regnety_004',\n",
       " 'regnety_006',\n",
       " 'regnety_008',\n",
       " 'regnety_016',\n",
       " 'regnety_032',\n",
       " 'regnety_040',\n",
       " 'regnety_064',\n",
       " 'regnety_080',\n",
       " 'regnety_120',\n",
       " 'regnety_160',\n",
       " 'regnety_320',\n",
       " 'repvgg_a2',\n",
       " 'repvgg_b0',\n",
       " 'repvgg_b1',\n",
       " 'repvgg_b1g4',\n",
       " 'repvgg_b2',\n",
       " 'repvgg_b2g4',\n",
       " 'repvgg_b3',\n",
       " 'repvgg_b3g4',\n",
       " 'res2net50_14w_8s',\n",
       " 'res2net50_26w_4s',\n",
       " 'res2net50_26w_6s',\n",
       " 'res2net50_26w_8s',\n",
       " 'res2net50_48w_2s',\n",
       " 'res2net101_26w_4s',\n",
       " 'res2next50',\n",
       " 'resmlp_12_224',\n",
       " 'resmlp_12_distilled_224',\n",
       " 'resmlp_24_224',\n",
       " 'resmlp_24_distilled_224',\n",
       " 'resmlp_36_224',\n",
       " 'resmlp_36_distilled_224',\n",
       " 'resmlp_big_24_224',\n",
       " 'resmlp_big_24_224_in22ft1k',\n",
       " 'resmlp_big_24_distilled_224',\n",
       " 'resnest14d',\n",
       " 'resnest26d',\n",
       " 'resnest50d',\n",
       " 'resnest50d_1s4x24d',\n",
       " 'resnest50d_4s2x40d',\n",
       " 'resnest101e',\n",
       " 'resnest200e',\n",
       " 'resnest269e',\n",
       " 'resnet18',\n",
       " 'resnet18d',\n",
       " 'resnet26',\n",
       " 'resnet26d',\n",
       " 'resnet26t',\n",
       " 'resnet34',\n",
       " 'resnet34d',\n",
       " 'resnet50',\n",
       " 'resnet50d',\n",
       " 'resnet50t',\n",
       " 'resnet51q',\n",
       " 'resnet61q',\n",
       " 'resnet101',\n",
       " 'resnet101d',\n",
       " 'resnet152',\n",
       " 'resnet152d',\n",
       " 'resnet200',\n",
       " 'resnet200d',\n",
       " 'resnetblur18',\n",
       " 'resnetblur50',\n",
       " 'resnetrs50',\n",
       " 'resnetrs101',\n",
       " 'resnetrs152',\n",
       " 'resnetrs200',\n",
       " 'resnetrs270',\n",
       " 'resnetrs350',\n",
       " 'resnetrs420',\n",
       " 'resnetv2_50',\n",
       " 'resnetv2_50d',\n",
       " 'resnetv2_50t',\n",
       " 'resnetv2_50x1_bit_distilled',\n",
       " 'resnetv2_50x1_bitm',\n",
       " 'resnetv2_50x1_bitm_in21k',\n",
       " 'resnetv2_50x3_bitm',\n",
       " 'resnetv2_50x3_bitm_in21k',\n",
       " 'resnetv2_101',\n",
       " 'resnetv2_101d',\n",
       " 'resnetv2_101x1_bitm',\n",
       " 'resnetv2_101x1_bitm_in21k',\n",
       " 'resnetv2_101x3_bitm',\n",
       " 'resnetv2_101x3_bitm_in21k',\n",
       " 'resnetv2_152',\n",
       " 'resnetv2_152d',\n",
       " 'resnetv2_152x2_bit_teacher',\n",
       " 'resnetv2_152x2_bit_teacher_384',\n",
       " 'resnetv2_152x2_bitm',\n",
       " 'resnetv2_152x2_bitm_in21k',\n",
       " 'resnetv2_152x4_bitm',\n",
       " 'resnetv2_152x4_bitm_in21k',\n",
       " 'resnext50_32x4d',\n",
       " 'resnext50d_32x4d',\n",
       " 'resnext101_32x4d',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'rexnet_100',\n",
       " 'rexnet_130',\n",
       " 'rexnet_150',\n",
       " 'rexnet_200',\n",
       " 'rexnetr_100',\n",
       " 'rexnetr_130',\n",
       " 'rexnetr_150',\n",
       " 'rexnetr_200',\n",
       " 'selecsls42',\n",
       " 'selecsls42b',\n",
       " 'selecsls60',\n",
       " 'selecsls60b',\n",
       " 'selecsls84',\n",
       " 'semnasnet_050',\n",
       " 'semnasnet_075',\n",
       " 'semnasnet_100',\n",
       " 'semnasnet_140',\n",
       " 'senet154',\n",
       " 'seresnet18',\n",
       " 'seresnet34',\n",
       " 'seresnet50',\n",
       " 'seresnet50t',\n",
       " 'seresnet101',\n",
       " 'seresnet152',\n",
       " 'seresnet152d',\n",
       " 'seresnet200d',\n",
       " 'seresnet269d',\n",
       " 'seresnext26d_32x4d',\n",
       " 'seresnext26t_32x4d',\n",
       " 'seresnext26tn_32x4d',\n",
       " 'seresnext50_32x4d',\n",
       " 'seresnext101_32x4d',\n",
       " 'seresnext101_32x8d',\n",
       " 'skresnet18',\n",
       " 'skresnet34',\n",
       " 'skresnet50',\n",
       " 'skresnet50d',\n",
       " 'skresnext50_32x4d',\n",
       " 'spnasnet_100',\n",
       " 'ssl_resnet18',\n",
       " 'ssl_resnet50',\n",
       " 'ssl_resnext50_32x4d',\n",
       " 'ssl_resnext101_32x4d',\n",
       " 'ssl_resnext101_32x8d',\n",
       " 'ssl_resnext101_32x16d',\n",
       " 'swin_base_patch4_window7_224',\n",
       " 'swin_base_patch4_window7_224_in22k',\n",
       " 'swin_base_patch4_window12_384',\n",
       " 'swin_base_patch4_window12_384_in22k',\n",
       " 'swin_large_patch4_window7_224',\n",
       " 'swin_large_patch4_window7_224_in22k',\n",
       " 'swin_large_patch4_window12_384',\n",
       " 'swin_large_patch4_window12_384_in22k',\n",
       " 'swin_small_patch4_window7_224',\n",
       " 'swin_tiny_patch4_window7_224',\n",
       " 'swinnet26t_256',\n",
       " 'swinnet50ts_256',\n",
       " 'swsl_resnet18',\n",
       " 'swsl_resnet50',\n",
       " 'swsl_resnext50_32x4d',\n",
       " 'swsl_resnext101_32x4d',\n",
       " 'swsl_resnext101_32x8d',\n",
       " 'swsl_resnext101_32x16d',\n",
       " 'tf_efficientnet_b0',\n",
       " 'tf_efficientnet_b0_ap',\n",
       " 'tf_efficientnet_b0_ns',\n",
       " 'tf_efficientnet_b1',\n",
       " 'tf_efficientnet_b1_ap',\n",
       " 'tf_efficientnet_b1_ns',\n",
       " 'tf_efficientnet_b2',\n",
       " 'tf_efficientnet_b2_ap',\n",
       " 'tf_efficientnet_b2_ns',\n",
       " 'tf_efficientnet_b3',\n",
       " 'tf_efficientnet_b3_ap',\n",
       " 'tf_efficientnet_b3_ns',\n",
       " 'tf_efficientnet_b4',\n",
       " 'tf_efficientnet_b4_ap',\n",
       " 'tf_efficientnet_b4_ns',\n",
       " 'tf_efficientnet_b5',\n",
       " 'tf_efficientnet_b5_ap',\n",
       " 'tf_efficientnet_b5_ns',\n",
       " 'tf_efficientnet_b6',\n",
       " 'tf_efficientnet_b6_ap',\n",
       " 'tf_efficientnet_b6_ns',\n",
       " 'tf_efficientnet_b7',\n",
       " 'tf_efficientnet_b7_ap',\n",
       " 'tf_efficientnet_b7_ns',\n",
       " 'tf_efficientnet_b8',\n",
       " 'tf_efficientnet_b8_ap',\n",
       " 'tf_efficientnet_cc_b0_4e',\n",
       " 'tf_efficientnet_cc_b0_8e',\n",
       " 'tf_efficientnet_cc_b1_8e',\n",
       " 'tf_efficientnet_el',\n",
       " 'tf_efficientnet_em',\n",
       " 'tf_efficientnet_es',\n",
       " 'tf_efficientnet_l2_ns',\n",
       " 'tf_efficientnet_l2_ns_475',\n",
       " 'tf_efficientnet_lite0',\n",
       " 'tf_efficientnet_lite1',\n",
       " 'tf_efficientnet_lite2',\n",
       " 'tf_efficientnet_lite3',\n",
       " 'tf_efficientnet_lite4',\n",
       " 'tf_efficientnetv2_b0',\n",
       " 'tf_efficientnetv2_b1',\n",
       " 'tf_efficientnetv2_b2',\n",
       " 'tf_efficientnetv2_b3',\n",
       " 'tf_efficientnetv2_l',\n",
       " 'tf_efficientnetv2_l_in21ft1k',\n",
       " 'tf_efficientnetv2_l_in21k',\n",
       " 'tf_efficientnetv2_m',\n",
       " 'tf_efficientnetv2_m_in21ft1k',\n",
       " 'tf_efficientnetv2_m_in21k',\n",
       " 'tf_efficientnetv2_s',\n",
       " 'tf_efficientnetv2_s_in21ft1k',\n",
       " 'tf_efficientnetv2_s_in21k',\n",
       " 'tf_inception_v3',\n",
       " 'tf_mixnet_l',\n",
       " 'tf_mixnet_m',\n",
       " 'tf_mixnet_s',\n",
       " 'tf_mobilenetv3_large_075',\n",
       " 'tf_mobilenetv3_large_100',\n",
       " 'tf_mobilenetv3_large_minimal_100',\n",
       " 'tf_mobilenetv3_small_075',\n",
       " 'tf_mobilenetv3_small_100',\n",
       " 'tf_mobilenetv3_small_minimal_100',\n",
       " 'tnt_b_patch16_224',\n",
       " 'tnt_s_patch16_224',\n",
       " 'tresnet_l',\n",
       " 'tresnet_l_448',\n",
       " 'tresnet_m',\n",
       " 'tresnet_m_448',\n",
       " 'tresnet_m_miil_in21k',\n",
       " 'tresnet_xl',\n",
       " 'tresnet_xl_448',\n",
       " 'tv_densenet121',\n",
       " 'tv_resnet34',\n",
       " 'tv_resnet50',\n",
       " 'tv_resnet101',\n",
       " 'tv_resnet152',\n",
       " 'tv_resnext50_32x4d',\n",
       " 'twins_pcpvt_base',\n",
       " 'twins_pcpvt_large',\n",
       " 'twins_pcpvt_small',\n",
       " 'twins_svt_base',\n",
       " 'twins_svt_large',\n",
       " 'twins_svt_small',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'visformer_small',\n",
       " 'visformer_tiny',\n",
       " 'vit_base_patch16_224',\n",
       " 'vit_base_patch16_224_in21k',\n",
       " 'vit_base_patch16_224_miil',\n",
       " 'vit_base_patch16_224_miil_in21k',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_base_patch32_224',\n",
       " 'vit_base_patch32_224_in21k',\n",
       " 'vit_base_patch32_384',\n",
       " 'vit_base_r26_s32_224',\n",
       " 'vit_base_r50_s16_224',\n",
       " 'vit_base_r50_s16_224_in21k',\n",
       " 'vit_base_r50_s16_384',\n",
       " 'vit_base_resnet26d_224',\n",
       " 'vit_base_resnet50_224_in21k',\n",
       " 'vit_base_resnet50_384',\n",
       " 'vit_base_resnet50d_224',\n",
       " 'vit_huge_patch14_224_in21k',\n",
       " 'vit_large_patch16_224',\n",
       " 'vit_large_patch16_224_in21k',\n",
       " 'vit_large_patch16_384',\n",
       " 'vit_large_patch32_224',\n",
       " 'vit_large_patch32_224_in21k',\n",
       " 'vit_large_patch32_384',\n",
       " 'vit_large_r50_s32_224',\n",
       " 'vit_large_r50_s32_224_in21k',\n",
       " 'vit_large_r50_s32_384',\n",
       " 'vit_small_patch16_224',\n",
       " 'vit_small_patch16_224_in21k',\n",
       " 'vit_small_patch16_384',\n",
       " 'vit_small_patch32_224',\n",
       " 'vit_small_patch32_224_in21k',\n",
       " 'vit_small_patch32_384',\n",
       " 'vit_small_r26_s32_224',\n",
       " 'vit_small_r26_s32_224_in21k',\n",
       " 'vit_small_r26_s32_384',\n",
       " 'vit_small_resnet26d_224',\n",
       " 'vit_small_resnet50d_s16_224',\n",
       " 'vit_tiny_patch16_224',\n",
       " 'vit_tiny_patch16_224_in21k',\n",
       " 'vit_tiny_patch16_384',\n",
       " 'vit_tiny_r_s16_p8_224',\n",
       " 'vit_tiny_r_s16_p8_224_in21k',\n",
       " 'vit_tiny_r_s16_p8_384',\n",
       " 'vovnet39a',\n",
       " 'vovnet57a',\n",
       " 'wide_resnet50_2',\n",
       " 'wide_resnet101_2',\n",
       " 'xception',\n",
       " 'xception41',\n",
       " 'xception65',\n",
       " 'xception71']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e6be889-ed07-4238-931d-ae632e5ed425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89d01317-b839-4c53-bd9f-a78fe1f1c733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([1, 64, 64, 64])\n",
      "torch.Size([1, 64, 32, 32])\n",
      "torch.Size([1, 128, 16, 16])\n",
      "torch.Size([1, 256, 8, 8])\n",
      "torch.Size([1, 512, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "encoder = get_encoder(\n",
    "            \"resnet34\",\n",
    "            in_channels=1,\n",
    "            depth=5,\n",
    "            weights=None,\n",
    "        )\n",
    "x = torch.randn(1, 1, 128, 128)\n",
    "out = encoder(x)\n",
    "for feature in out:\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9f846ee-b8c7-40dc-84ae-212c3d92af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 128, 256, 512)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0370ebf-d0d3-4380-bf98-76cb136cd8ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 416, 3, 3], expected input[1, 304, 8, 8] to have 416 channels, but got 304 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_402/3153781502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# model = CustomUnet(encoder_name=\"resnet18\", encoder_weights=None, in_channels=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 453\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 416, 3, 3], expected input[1, 304, 8, 8] to have 416 channels, but got 304 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CustomUnet(smp.Unet):\n",
    "    \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_batchnorm: bool = True,\n",
    "        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if encoder_name == 'tf_efficientnetv2_b0':\n",
    "            self.encoder = smp.encoders.TimmUniversalEncoder(encoder_name, in_channels=in_channels, depth=encoder_depth, pretrained=encoder_weights is not None)\n",
    "            encoder_channels = [1, 24, 48, 64, 160, 256]\n",
    "            \n",
    "        else:  \n",
    "            self.encoder = get_encoder(\n",
    "                encoder_name,\n",
    "                in_channels=in_channels,\n",
    "                depth=encoder_depth,\n",
    "                weights=encoder_weights,\n",
    "            )\n",
    "            encoder_channels = self.encoder.out_channels\n",
    "        \n",
    "        \n",
    "\n",
    "        self.decoder = UnetDecoder(\n",
    "            encoder_channels=encoder_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_batchnorm=decoder_use_batchnorm,\n",
    "            center=True if encoder_name.startswith(\"vgg\") else False,\n",
    "            attention_type=decoder_attention_type,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"u-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "model = CustomUnet(encoder_name=\"tf_efficientnetv2_b0\", encoder_weights=None, decoder_channels=[256, 128, 64, 32, 16], in_channels=1)\n",
    "# model = CustomUnet(encoder_name=\"resnet18\", encoder_weights=None, in_channels=1)\n",
    "\n",
    "out = model(torch.randn(1, 1, 128, 128))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "752ce7a8-b09f-4ad9-b954-8e62f411201a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomUnet(\n",
       "  (encoder): TimmUniversalEncoder(\n",
       "    (model): EfficientNetFeatures(\n",
       "      (conv_stem): Conv2dSame(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): SiLU(inplace=True)\n",
       "      (blocks): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): ConvBnAct(\n",
       "            (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): EdgeResidual(\n",
       "            (conv_exp): Conv2dSame(16, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (se): Identity()\n",
       "            (conv_pwl): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): EdgeResidual(\n",
       "            (conv_exp): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (se): Identity()\n",
       "            (conv_pwl): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): EdgeResidual(\n",
       "            (conv_exp): Conv2dSame(32, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (se): Identity()\n",
       "            (conv_pwl): Conv2d(128, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): EdgeResidual(\n",
       "            (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (se): Identity()\n",
       "            (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2dSame(192, 192, kernel_size=(3, 3), stride=(2, 2), groups=192, bias=False)\n",
       "            (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(192, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(12, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (bn2): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv_pw): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (bn2): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(576, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (3): InvertedResidual(\n",
       "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (4): InvertedResidual(\n",
       "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2dSame(672, 672, kernel_size=(3, 3), stride=(2, 2), groups=672, bias=False)\n",
       "            (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (3): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (4): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (5): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (6): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (7): InvertedResidual(\n",
       "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): SiLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(304, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(304, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(80, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(24, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14d29754-3c5b-4b76-95b2-31947b11ee77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342070105"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "from efficientnet_pytorch.utils import Conv2dStaticSamePadding\n",
    "from timm.models.layers import Conv2dSame\n",
    "\n",
    "def get_same_padding(x: int, k: int, s: int, d: int):\n",
    "    return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n",
    "\n",
    "def pad_same(x, k: List[int], s: List[int], d: List[int] = (1, 1, 1), value: float = 0):\n",
    "    ip, ih, iw = x.size()[-3:]\n",
    "    pad_p, pad_h, pad_w = get_same_padding(ip, k[0], s[0], d[0]), get_same_padding(ih, k[1], s[1], d[1]), get_same_padding(iw, k[2], s[2], d[2])\n",
    "    if pad_p > 0 or pad_h > 0 or pad_w > 0:\n",
    "        x = F.pad(x, [pad_p // 2, pad_p - pad_p // 2, pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2], value=value)\n",
    "    return x\n",
    "\n",
    "def conv3d_same(\n",
    "        x, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, stride: Tuple[int, int] = (1, 1, 1),\n",
    "        padding: Tuple[int, int] = (0, 0, 0), dilation: Tuple[int, int] = (1, 1, 1), groups: int = 1):\n",
    "    x = pad_same(x, weight.shape[-3:], stride, dilation)\n",
    "    return F.conv3d(x, weight, bias, stride, (0, 0, 0), dilation, groups)\n",
    "\n",
    "\n",
    "class Conv3dSame(nn.Conv2d):\n",
    "    \"\"\" Tensorflow like 'SAME' convolution wrapper for 2D convolutions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(Conv3dSame, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return conv3d_same(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    new_module = module\n",
    "    if isinstance(module, Conv2dStaticSamePadding):\n",
    "        # print(module.static_padding)\n",
    "        if isinstance(module.static_padding, nn.ZeroPad2d):\n",
    "            padding = module.static_padding.padding + module.static_padding.padding[:2]\n",
    "            # print(padding)\n",
    "        else:\n",
    "            padding = 0\n",
    "            \n",
    "        new_module = nn.Sequential(\n",
    "                    nn.ConstantPad3d(padding, 0.),\n",
    "                    nn.Conv3d(module.in_channels,\n",
    "                               module.out_channels,\n",
    "                               kernel_size=module.kernel_size[0],\n",
    "                               stride=module.stride[0],\n",
    "                               padding=0,\n",
    "                               bias=module.bias is not None)\n",
    "        )\n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        new_module = Conv3dSame(module.in_channels,\n",
    "                               module.out_channels,\n",
    "                               kernel_size=module.kernel_size[0],\n",
    "                               stride=module.stride[0],\n",
    "                               padding=0,\n",
    "                               bias=module.bias is not None\n",
    "                               )\n",
    "        \n",
    "    elif isinstance(module, nn.Conv2d):\n",
    "        # print(module.kernel_size)\n",
    "        # print(module.bias)\n",
    "        # print(module.kernel_size[0], module.padding[0])\n",
    "        new_module = nn.Conv3d(module.in_channels,\n",
    "                               module.out_channels,\n",
    "                               kernel_size=module.kernel_size[0],\n",
    "                               stride=module.stride[0],\n",
    "                               padding=module.padding[0],\n",
    "                               bias=module.bias is not None)\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        new_module = nn.BatchNorm3d(module.num_features,\n",
    "                                    module.eps,\n",
    "                                    module.affine,\n",
    "                                    module.track_running_stats)\n",
    "    elif isinstance(module, nn.MaxPool2d):\n",
    "        new_module = nn.MaxPool3d(kernel_size=module.kernel_size,\n",
    "                                  stride=module.stride,\n",
    "                                  padding=module.padding,\n",
    "                                  dilation=module.dilation,\n",
    "                                  ceil_mode=module.ceil_mode)\n",
    "    elif isinstance(module, nn.AdaptiveAvgPool2d):\n",
    "        new_module = nn.AdaptiveAvgPool3d(module.output_size)\n",
    "\n",
    "    for name, child_module in new_module.named_children():\n",
    "        setattr(new_module, name, convert_3d(child_module))\n",
    "\n",
    "    return new_module\n",
    "\n",
    "\n",
    "model_3d = convert_3d(model)\n",
    "total_params = sum(p.numel() for p in model_3d.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "932dab17-9fbe-4b2c-ac7d-310e98e2c10f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 6 but got size 7 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48919/935903941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 6 but got size 7 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 128, 128, 128)\n",
    "out = model_3d(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cb371-a6de-496b-8ade-57986d6409cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
